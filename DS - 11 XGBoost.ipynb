{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost: eXtreme Gradient Boosting\n",
    "* Gradient Boosting algorithm also called GBM (Gradient Boosting Machine) including the learning rate\n",
    "* Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels\n",
    "* Extreme = because of engineering goal to make it super fast\n",
    "* Regularized Gradient Boosting with both L1 and L2 regularization\n",
    "\n",
    "**Nuances:**\n",
    "* Labels need to be 0.. n-1 classes\n",
    "* 2 APIs: xgb.train or xgb.XGBClassifier().fit - both APIs will have different parameter names (e.g. learning_rate or eta)\n",
    "* Default parameter values: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "\n",
    "**References:**\n",
    "* https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "* https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import auc, confusion_matrix, roc_auc_score, f1_score, plot_confusion_matrix\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBSVM / SVMLIGHT file format:\n",
    "* XGBoost currently supports two text formats for ingesting data: LIBSVM and CSV. [Link](https://xgboost.readthedocs.io/en/latest/tutorials/input_format.html)\n",
    "* LIBSVM/SVMLIGHT format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(dump_svmlight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=12)\n",
    "\n",
    "dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\n",
    "dump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)\n",
    "dtrain_svm = xgb.DMatrix('dtrain.svm')\n",
    "dtest_svm = xgb.DMatrix('dtest.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[1] = 0\n",
    "# np.savetxt(\"temp.csv\", X_train, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API option 1: TRAIN\n",
    "* parameter names are not consistent with sklearn (eta, num_round etc.)\n",
    "* requires DMatrix for both train and test\n",
    "* eval_metric part of train()'s params\n",
    "\n",
    "### Example - IRIS (because, why not!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=None, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None)\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round: int\n",
      "        Number of boosting iterations.\n",
      "    evals: list of pairs (DMatrix, string)\n",
      "        List of validation sets for which metrics will evaluated during training.\n",
      "        Validation metrics will help us track the performance of the model.\n",
      "    obj : function\n",
      "        Customized objective function.\n",
      "    feval : function\n",
      "        Customized evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Validation metric needs to improve at least once in\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        The method returns the model from the last iteration (not the best one).  Use\n",
      "        custom callback or model slicing if the best model is desired.\n",
      "        If there's more than one item in **evals**, the last entry will be used for early\n",
      "        stopping.\n",
      "        If there's more than one metric in the **eval_metric** parameter given in\n",
      "        **params**, the last metric will be used for early stopping.\n",
      "        If early stopping occurs, the model will have three additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.  Use\n",
      "        ``bst.best_ntree_limit`` to get the correct value if ``num_parallel_tree`` and/or\n",
      "        ``num_class`` appears in the parameters.  ``best_ntree_limit`` is the result of\n",
      "        ``num_parallel_tree * best_iteration``.\n",
      "    evals_result: dict\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval : bool or int\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9305832e-01 5.6063612e-03 1.3352670e-03]\n",
      " [7.6278631e-04 1.2921481e-03 9.9794501e-01]\n",
      " [9.9207652e-01 5.0041736e-03 2.9192679e-03]\n",
      " [3.3957248e-03 9.9417937e-01 2.4249505e-03]\n",
      " [1.5521281e-03 3.4555295e-03 9.9499232e-01]]\n",
      "[0 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'max_depth': 3,                 # the maximum depth of each tree\n",
    "    'eta': 0.3,                     # the training step for each iteration\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training (softmax = class, softprob = probability)\n",
    "    'num_class': 3,                 # the number of classes that exist in this datset\n",
    "    'eval_metric':'mlogloss',\n",
    "#     'num_round':20\n",
    "}                 \n",
    "\n",
    "model1 = xgb.train(param, dtrain, num_boost_round=50)\n",
    "\n",
    "preds = model1.predict(dtest)\n",
    "print(preds[0:5])\n",
    "\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print(best_preds[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, best_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API option 2: FIT (sklearn wrapper)\n",
    "* DMatrix is not needed\n",
    "* params as part of classifier instance\n",
    "* eval_metric as part of classifier parameter or model.fit()\n",
    "\n",
    "### Example - Palmer Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : int\n",
      " |          Number of boosting rounds.\n",
      " |      use_label_encoder : bool\n",
      " |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n",
      " |          code, we recommend that you set this parameter to False.\n",
      " |  \n",
      " |      max_depth : int\n",
      " |          Maximum tree depth for base learners.\n",
      " |      learning_rate : float\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : int\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : string or callable\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: string\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: string\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      " |          is set to default, XGBoost will choose the most conservative option\n",
      " |          available.  It's recommended to study this option from parameters\n",
      " |          document.\n",
      " |      n_jobs : int\n",
      " |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      " |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      " |          balance the threads.  Creating thread contention will significantly slow down both\n",
      " |          algorithms.\n",
      " |      gamma : float\n",
      " |          Minimum loss reduction required to make a further partition on a leaf\n",
      " |          node of the tree.\n",
      " |      min_child_weight : float\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : float\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : float\n",
      " |          Subsample ratio of the training instance.\n",
      " |      colsample_bytree : float\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : float\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : float\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : float (xgb's alpha)\n",
      " |          L1 regularization term on weights\n",
      " |      reg_lambda : float (xgb's lambda)\n",
      " |          L2 regularization term on weights\n",
      " |      scale_pos_weight : float\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score:\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : int\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: int\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : str\n",
      " |          Constraint of variable monotonicity.  See tutorial for more\n",
      " |          information.\n",
      " |      interaction_constraints : str\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      " |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      " |          that are allowed to interact with each other.  See tutorial for more\n",
      " |          information\n",
      " |      importance_type: string, default \"gain\"\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      " |      gpu_id :\n",
      " |          Device ordinal.\n",
      " |      validate_parameters :\n",
      " |          Give warnings for unknown parameter.\n",
      " |  \n",
      " |      \\*\\*kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      " |          parameters can be found here:\n",
      " |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective='binary:logistic', use_label_encoder=True, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, base_margin_eval_set=None, feature_weights=None, callbacks=None)\n",
      " |      Fit gradient boosting classifier.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          instance weights\n",
      " |      base_margin : array_like\n",
      " |          global bias for each instance.\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      eval_metric : str, list of str, or callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.rst.\n",
      " |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      " |          to use.\n",
      " |          If callable, a custom evaluation metric. The call signature is\n",
      " |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      " |          that you may need to call the ``get_label`` method. It must return a str,\n",
      " |          value pair where the str is a name for the evaluation and value is the value\n",
      " |          of the evaluation function. The callable custom objective is always minimized.\n",
      " |      early_stopping_rounds : int\n",
      " |          Activates early stopping. Validation metric needs to improve at least once in\n",
      " |          every **early_stopping_rounds** round(s) to continue training.\n",
      " |          Requires at least one item in **eval_set**.\n",
      " |          The method returns the model from the last iteration (not the best one).\n",
      " |          If there's more than one item in **eval_set**, the last entry will be used\n",
      " |          for early stopping.\n",
      " |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      " |          used for early stopping.\n",
      " |          If early stopping occurs, the model will have three additional fields:\n",
      " |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      " |          measured on the validation set to stderr.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set : list, optional\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights: array_like\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      " |          `exact` tree methods.\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      " |                                                      save_best=True)]\n",
      " |  \n",
      " |  predict(self, X, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None, iteration_range: Union[Tuple[int, int], NoneType] = None)\n",
      " |      Predict with `X`.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Data to predict with\n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  predict_proba(self, X, ntree_limit=None, validate_features=False, base_margin=None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
      " |      Predict the probability of each `X` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      " |          probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  apply(self, X, ntree_limit: int = 0, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self)\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      \n",
      " |      The model is loaded from an XGBoost internal format which is universal\n",
      " |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      " |      Python Booster object (such as feature names) will not be loaded.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |  \n",
      " |  save_model(self, fname: str)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal\n",
      " |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      " |      Python Booster object (such as feature names) will not be saved.\n",
      " |      \n",
      " |        .. note::\n",
      " |      \n",
      " |          See:\n",
      " |      \n",
      " |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |  \n",
      " |  best_ntree_limit\n",
      " |  \n",
      " |  best_score\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]``\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.XGBClassifier)\n",
    "# eval_metric options: https://xgboost.readthedocs.io/en/stable/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1  Adelie  Torgersen              39.5             17.4              186.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adelie' 'Chinstrap' 'Gentoo']\n",
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "      <th>sex_MALE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0              39.1             18.7              181.0       3750.0   \n",
       "1              39.5             17.4              186.0       3800.0   \n",
       "\n",
       "   island_Dream  island_Torgersen  sex_MALE  \n",
       "0             0                 1         1  \n",
       "1             0                 1         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233, 7) (100, 7)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../GitHub/Python-Data-Science/Data/penguins_size.csv')\n",
    "df.dropna(inplace=True)   # reviewed in another notebook\n",
    "df[df['sex']=='.']\n",
    "df = df.drop(index=336)\n",
    "# df.info()\n",
    "display(df.head(2))\n",
    "\n",
    "X = df.drop(columns='species')\n",
    "y = df['species'].copy()\n",
    "print(np.unique(y))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_ordinal = encoder.fit_transform(y)\n",
    "print(np.unique(y_ordinal))\n",
    "\n",
    "X_dummy = pd.get_dummies(X, drop_first=True)\n",
    "display(X_dummy.head(2))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dummy, y_ordinal, test_size=0.3, random_state=101)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can handle missing values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditg\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "X_train.iloc[0,1] = np.NaN\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgb.XGBClassifier(max_depth=3, learning_rate=0.3, use_label_encoder=False, eval_metric='merror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None,\n",
       "              eval_metric='merror', gamma=None, gpu_id=None,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.3, max_delta_step=None, max_depth=3,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              use_label_encoder=False, validate_parameters=None,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7) (100,)\n",
      "[[39  1  0]\n",
      " [ 1 26  0]\n",
      " [ 0  0 33]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditg\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, y_test.shape)\n",
    "val_test = [(X_test, y_test)]\n",
    "\n",
    "model2.fit(X_train, y_train) #, eval_set=val_test)\n",
    "y_pred = model2.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))                 #Better than decision tree, same as RF & SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model2, X_test, y_test);   #won't work with 1st API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for Tuning Hyperparameters\n",
    "* Either through xgb.train or sklearn's GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditg\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, eval_metric='mlogloss',\n",
       "                                     gamma=0, gpu_id=-1, importance_type='gain',\n",
       "                                     interaction_constraints='',\n",
       "                                     learning_rate=0.3, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=nan, monotone_constraints='()',\n",
       "                                     n_estimators=100, n_jobs=12,\n",
       "                                     num_parallel_tree=1,\n",
       "                                     objective='multi:softprob', random_state=0,\n",
       "                                     reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=None, subsample=1,\n",
       "                                     tree_method='exact',\n",
       "                                     use_label_encoder=False,\n",
       "                                     validate_parameters=1, verbosity=None),\n",
       "             param_grid={'colsample_bynode': [0.5],\n",
       "                         'learning_rate': [0.02, 0.5, 0.1, 1],\n",
       "                         'max_depth': [3, 6], 'n_estimators': [100, 200]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = xgb.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "param_grid = {'colsample_bynode':[0.5],\n",
    "              'n_estimators':[100, 200],\n",
    "              'max_depth': [3,6],\n",
    "              'learning_rate': [0.02, 0.5, 0.1, 1]}\n",
    "\n",
    "grid  = GridSearchCV(model2, param_grid)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bynode': 0.5,\n",
       " 'learning_rate': 0.02,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "plot_confusion_matrix(grid, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Santander Customer Transaction Prediction\n",
    "* https://www.kaggle.com/c/santander-customer-transaction-prediction/data?select=test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 202) (200000, 201)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   var_8   var_9  var_10   var_11   var_12   var_13  var_14  \\\n",
       "0  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745  8.7989   \n",
       "1  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135  5.4345   \n",
       "2  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124  7.5244   \n",
       "3  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704  6.4569   \n",
       "4  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895  7.7894   \n",
       "\n",
       "    var_15   var_16   var_17   var_18   var_19   var_20   var_21   var_22  \\\n",
       "0  14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   2.5791   \n",
       "1  13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   8.5524   \n",
       "2  14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   1.2145   \n",
       "3  14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   6.8202   \n",
       "4  15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608  10.1102   \n",
       "\n",
       "   var_23   var_24   var_25   var_26  var_27  var_28  var_29   var_30  \\\n",
       "0  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965  -0.3085   \n",
       "1  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481  -9.2987   \n",
       "2  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439 -11.1025   \n",
       "3  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407 -15.2398   \n",
       "4  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352  -3.1582   \n",
       "\n",
       "    var_31  var_32   var_33   var_34   var_35  var_36  var_37   var_38  \\\n",
       "0  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   4.6667   \n",
       "1   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434  10.8160   \n",
       "2  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346  10.8917   \n",
       "3  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   9.1738   \n",
       "4   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184  19.3038   \n",
       "\n",
       "   var_39   var_40   var_41   var_42   var_43   var_44   var_45   var_46  \\\n",
       "0  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418  -7.0170   5.9226   \n",
       "1  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444 -47.3797   7.3718   \n",
       "2  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006  -7.9078  11.1405   \n",
       "3  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485 -24.6840  12.7404   \n",
       "4  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087 -65.4863  13.8657   \n",
       "\n",
       "    var_47   var_48   var_49   var_50   var_51  var_52  var_53  var_54  \\\n",
       "0 -14.2136  16.0283   5.3253  12.9194  29.0460 -0.6940  5.1736 -0.7474   \n",
       "1   0.1948  34.4014  25.7037  11.8343  13.2256 -4.1083  6.6885 -8.0946   \n",
       "2  -5.7864  20.7477   6.8874  12.9143  19.5856  0.7268  6.4059  9.3124   \n",
       "3 -35.1659   0.7613   8.3838  12.6832   9.5503  1.7895  5.2091  8.0913   \n",
       "4   0.0444  -0.1346  14.4268  13.3273  10.4857 -1.4367  5.7555 -8.5414   \n",
       "\n",
       "    var_55   var_56  var_57  var_58   var_59   var_60   var_61  var_62  \\\n",
       "0  14.8322  11.2668  5.3822  2.0183  10.1166  16.1828   4.9590  2.0771   \n",
       "1  18.5995  19.3219  7.0118  1.9210   8.8682   8.0109  -7.2417  1.7944   \n",
       "2   6.2846  15.6372  5.8200  1.1000   9.1854  12.5963 -10.3734  0.8748   \n",
       "3  12.3972  14.4698  6.5850  3.3164   9.4638  15.7820 -25.0222  3.4418   \n",
       "4  14.1482  16.9840  6.1812  1.9548   9.2048   8.6591 -27.7439 -0.4952   \n",
       "\n",
       "   var_63  var_64  var_65  var_66   var_67  var_68  var_69   var_70  var_71  \\\n",
       "0 -0.2154  8.6748  9.5319  5.8056  22.4321  5.0109 -4.7010  21.6374  0.5663   \n",
       "1 -1.3147  8.1042  1.5365  5.4007   7.9344  5.0220  2.2302  40.5632  0.5134   \n",
       "2  5.8042  3.7163 -1.1016  7.3667   9.8565  5.0228 -5.7828   2.3612  0.8520   \n",
       "3 -4.3923  8.6464  6.3072  5.6221  23.6143  5.0220 -3.9989   4.0462  0.2500   \n",
       "4 -1.7839  5.2670 -4.3205  6.9860   1.6184  5.0301 -3.2431  40.1236  0.7737   \n",
       "\n",
       "   var_72   var_73   var_74   var_75   var_76   var_77  var_78   var_79  \\\n",
       "0  5.1999   8.8600  43.1127  18.3816  -2.3440  23.4104  6.5199  12.1983   \n",
       "1  3.1701  20.1068   7.7841   7.0529   3.2709  23.4822  5.5075  13.7814   \n",
       "2  6.3577  12.1719  19.7312  19.4465   4.5048  23.2378  6.3191  12.8046   \n",
       "3  1.2516  24.4187   4.5290  15.4235  11.6875  23.6273  4.0806  15.2733   \n",
       "4 -0.7264   4.5886  -4.5346  23.3521   1.0273  19.1600  7.1734  14.3937   \n",
       "\n",
       "    var_80   var_81   var_82   var_83  var_84   var_85   var_86   var_87  \\\n",
       "0  13.6468  13.8372   1.3675   2.9423 -4.5213  21.4669   9.3225  16.4597   \n",
       "1   2.5462  18.1782   0.3683  -4.8210 -5.4850  13.7867 -13.5901  11.0993   \n",
       "2   7.4729  15.7811  13.3529  10.1852  5.4604  19.0773  -4.4577   9.5413   \n",
       "3   0.7839  10.5404   1.6212  -5.2896  1.6027  17.9762  -2.3174  15.6298   \n",
       "4   2.9598  13.3317  -9.2587  -6.7075  7.8984  14.5265   7.0799  20.1670   \n",
       "\n",
       "    var_88   var_89   var_90  var_91   var_92   var_93   var_94  var_95  \\\n",
       "0   7.9984  -1.7069 -21.4494  6.7806  11.0924   9.9913  14.8421  0.1812   \n",
       "1   7.9022  12.2301   0.4768  6.8852   8.0905  10.9631  11.7569 -1.2722   \n",
       "2  11.9052   2.1447 -22.4038  7.0883  14.1613  10.5080  14.2621  0.2647   \n",
       "3   4.5474   7.5509  -7.5866  7.0364  14.4027  10.7795   7.2887 -1.0930   \n",
       "4   8.0053   3.7954 -39.7997  7.0065   9.3627  10.4316  14.0553  0.0213   \n",
       "\n",
       "    var_96   var_97  var_98  var_99  var_100  var_101  var_102  var_103  \\\n",
       "0   8.9642  16.2572  2.1743 -3.4132   9.4763  13.3102  26.5376   1.4403   \n",
       "1  24.7876  26.6881  1.8944  0.6939 -13.6950   8.4068  35.4734   1.7093   \n",
       "2  20.4031  17.0360  1.6981 -0.0269  -0.3939  12.6317  14.8863   1.3854   \n",
       "3  11.3596  18.1486  2.8344  1.9480 -19.8592  22.5316  18.6129   1.3512   \n",
       "4  14.7246  35.2988  1.6844  0.6715 -22.9264  12.3562  17.3410   1.6940   \n",
       "\n",
       "   var_104  var_105  var_106  var_107  var_108  var_109  var_110  var_111  \\\n",
       "0  14.7100   6.0454   9.5426  17.1554  14.1104  24.3627   2.0323   6.7602   \n",
       "1  15.1866   2.6227   7.3412  32.0888  13.9550  13.0858   6.6203   7.1051   \n",
       "2  15.0284   3.9995   5.3683   8.6273  14.1963  20.3882   3.2304   5.7033   \n",
       "3   9.3291   4.2835  10.3907   7.0874  14.3256  14.4135   4.2827   6.9750   \n",
       "4   7.1179   5.1934   8.8230  10.6617  14.0837  28.2749  -0.1937   5.9654   \n",
       "\n",
       "   var_112  var_113  var_114  var_115  var_116  var_117  var_118  var_119  \\\n",
       "0   3.9141  -0.4851   2.5240   1.5093   2.5516  15.5752 -13.4221   7.2739   \n",
       "1   5.3523   8.5426   3.6159   4.1569   3.0454   7.8522 -11.5100   7.5109   \n",
       "2   4.5255   2.1929   3.1290   2.9044   1.1696  28.7632 -17.2738   2.1056   \n",
       "3   1.6480  11.6896   2.5762  -2.5459   5.3446  38.1015   3.5732   5.0988   \n",
       "4   1.0719   7.9923   2.9138  -3.6135   1.4684  25.6795  13.8224   4.7478   \n",
       "\n",
       "   var_120  var_121  var_122  var_123  var_124  var_125  var_126  var_127  \\\n",
       "0  16.0094   9.7268   0.8897   0.7754   4.2218  12.0039  13.8571  -0.7338   \n",
       "1  31.5899   9.5018   8.2736  10.1633   0.1225  12.5942  14.5697   2.4354   \n",
       "2  21.1613   8.9573   2.7768  -2.1746   3.6932  12.4653  14.1978  -2.5511   \n",
       "3  30.5644  11.3025   3.9618  -8.2464   2.7038  12.3441  12.5431  -1.3683   \n",
       "4  41.1037  12.7140   5.2964   9.7289   3.9370  12.1316  12.5815   7.0642   \n",
       "\n",
       "   var_128  var_129  var_130  var_131  var_132  var_133  var_134  var_135  \\\n",
       "0  -1.9245  15.4462  12.8287   0.3587   9.6508   6.5674   5.1726   3.1345   \n",
       "1   0.8194  16.5346  12.4205  -0.1780   5.7582   7.0513   1.9568  -8.9921   \n",
       "2  -0.9479  17.1092  11.5419   0.0975   8.8186   6.6231   3.9358 -11.7218   \n",
       "3   3.5974  13.9761  14.3003   1.0486   8.9500   7.1954  -1.1984   1.9586   \n",
       "4   5.6518  10.9346  11.4266   0.9442   7.7532   6.6173  -6.8304   6.4730   \n",
       "\n",
       "   var_136  var_137  var_138  var_139  var_140  var_141  var_142  var_143  \\\n",
       "0  29.4547  31.4045   2.8279  15.6599   8.3307  -5.6011  19.0614  11.2663   \n",
       "1   9.7797  18.1577  -1.9721  16.1622   3.6937   6.6803  -0.3243  12.2806   \n",
       "2  24.5437  15.5827   3.8212   8.6674   7.3834  -2.4438  10.2158   7.4844   \n",
       "3  27.5609  24.6065  -2.8233   8.9821   3.8873  15.9638  10.0142   7.8388   \n",
       "4  17.1728  25.8128   2.6791  13.9547   6.6289  -4.3965  11.7159  16.1080   \n",
       "\n",
       "   var_144  var_145  var_146  var_147  var_148  var_149  var_150  var_151  \\\n",
       "0   8.6989   8.3694  11.5659 -16.4727   4.0288  17.9244  18.5177  10.7800   \n",
       "1   8.6086  11.0738   8.9231  11.7700   4.2578  -4.4223  20.6294  14.8743   \n",
       "2   9.1104   4.3649  11.4934   1.7624   4.0714  -1.2681  14.3330   8.0088   \n",
       "3   9.9718   2.9253  10.4994   4.1622   3.7613   2.3701  18.0984  17.1765   \n",
       "4   7.6874   9.1570  11.5670 -12.7047   3.7574   9.9110  20.1461   1.2995   \n",
       "\n",
       "   var_152  var_153  var_154  var_155  var_156  var_157  var_158  var_159  \\\n",
       "0   9.0056  16.6964  10.4838   1.6573  12.1749 -13.1324  17.6054  11.5423   \n",
       "1   9.4317  16.7242  -0.5687   0.1898  12.2419  -9.6953  22.3949  10.6261   \n",
       "2   4.4015  14.1479  -5.1747   0.5778  14.5362  -1.7624  33.8820  11.6041   \n",
       "3   7.6508  18.2452  17.0336 -10.9370  12.0500  -1.2155  19.9750  12.3892   \n",
       "4   5.8493  19.8234   4.7022  10.6101  13.0021 -12.6068  27.0846   8.0913   \n",
       "\n",
       "   var_160  var_161  var_162  var_163  var_164  var_165  var_166  var_167  \\\n",
       "0  15.4576   5.3133   3.6159   5.0384   6.6760  12.6644   2.7004  -0.6975   \n",
       "1  29.4846   5.8683   3.8208  15.8348  -5.0121  15.1345   3.2003   9.3192   \n",
       "2  13.2070   5.8442   4.7086   5.7141  -1.0410  20.5092   3.2790  -5.5952   \n",
       "3  31.8833   5.9684   7.2084   3.8899 -11.0882  17.2502   2.5881  -2.7018   \n",
       "4  33.5107   5.6953   5.4663  18.2201   6.5769  21.2607   3.2304  -1.7759   \n",
       "\n",
       "   var_168  var_169  var_170  var_171  var_172  var_173  var_174  var_175  \\\n",
       "0   9.5981   5.4879  -4.7645  -8.4254  20.8773   3.1531  18.5618   7.7423   \n",
       "1   3.8821   5.7999   5.5378   5.0988  22.0330   5.5134  30.2645  10.4968   \n",
       "2   7.3176   5.7690  -7.0927  -3.9116   7.2569  -5.8234  25.6820  10.9202   \n",
       "3   0.5641   5.3430  -7.1541  -6.1920  18.2366  11.7134  14.7483   8.1013   \n",
       "4   3.1283   5.5518   1.4493  -2.6627  19.8056   2.3705  18.4685  16.3309   \n",
       "\n",
       "   var_176  var_177  var_178  var_179  var_180  var_181  var_182  var_183  \\\n",
       "0 -10.1245  13.7241  -3.5189   1.7202  -8.4051   9.0164   3.0657  14.3691   \n",
       "1  -7.2352  16.5721  -7.3477  11.0752  -5.5937   9.4878 -14.9100   9.4245   \n",
       "2  -0.3104   8.8438  -9.7009   2.4013  -4.2935   9.3908 -13.2648   3.1545   \n",
       "3  11.8771  13.9552 -10.4701   5.6961  -3.7546   8.4117   1.8986   7.2601   \n",
       "4  -3.3456  13.5261   1.7189   5.1743  -7.6938   9.7685   4.8910  12.2198   \n",
       "\n",
       "   var_184  var_185  var_186  var_187  var_188  var_189  var_190  var_191  \\\n",
       "0  25.8398   5.8764  11.8411 -19.7159  17.5743   0.5857   4.4354   3.9642   \n",
       "1  22.5441  -4.8622   7.6543 -15.9319  13.3175  -0.3566   7.6421   7.7214   \n",
       "2  23.0866  -5.3000   5.3745  -6.2660  10.1934  -0.8417   2.9057   9.7905   \n",
       "3  -0.4639  -0.0498   7.9336 -12.8279  12.4124   1.8489   4.4666   4.7433   \n",
       "4  11.8503  -7.8931   6.4209   5.9270  16.0201  -0.2829  -1.4905   9.5214   \n",
       "\n",
       "   var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0   3.1364   1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   2.5837  10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2   1.6704   1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3   0.7178   1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4  -0.1508   9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../GitHub/python-data-science/data/XGBoost/'\n",
    "df_train = pd.read_csv(path+'santander_train.csv')   \n",
    "df_test  = pd.read_csv(path+'santander_test.csv')   # no label\n",
    "\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "df_train.head()\n",
    "# ID_code = key\n",
    "# target = 0/1\n",
    "# var_0 to var_199 = numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()             \n",
    "# 1:10 imbalance.. not extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>7.567236</td>\n",
       "      <td>0.394340</td>\n",
       "      <td>-3.245596</td>\n",
       "      <td>14.023978</td>\n",
       "      <td>8.530232</td>\n",
       "      <td>7.537606</td>\n",
       "      <td>14.573126</td>\n",
       "      <td>9.333264</td>\n",
       "      <td>-5.696731</td>\n",
       "      <td>15.244013</td>\n",
       "      <td>12.438567</td>\n",
       "      <td>13.290894</td>\n",
       "      <td>17.257883</td>\n",
       "      <td>4.305430</td>\n",
       "      <td>3.019540</td>\n",
       "      <td>10.584400</td>\n",
       "      <td>13.667496</td>\n",
       "      <td>-4.055133</td>\n",
       "      <td>-1.137908</td>\n",
       "      <td>5.532980</td>\n",
       "      <td>5.053874</td>\n",
       "      <td>-7.687740</td>\n",
       "      <td>10.393046</td>\n",
       "      <td>-0.512886</td>\n",
       "      <td>14.774147</td>\n",
       "      <td>11.434250</td>\n",
       "      <td>3.842499</td>\n",
       "      <td>2.187230</td>\n",
       "      <td>5.868899</td>\n",
       "      <td>10.642131</td>\n",
       "      <td>0.662956</td>\n",
       "      <td>-6.725505</td>\n",
       "      <td>9.299858</td>\n",
       "      <td>11.222356</td>\n",
       "      <td>11.569954</td>\n",
       "      <td>8.948289</td>\n",
       "      <td>-12.699667</td>\n",
       "      <td>11.326488</td>\n",
       "      <td>-12.471737</td>\n",
       "      <td>14.704713</td>\n",
       "      <td>16.682499</td>\n",
       "      <td>12.740986</td>\n",
       "      <td>13.428912</td>\n",
       "      <td>-2.528816</td>\n",
       "      <td>6.008569</td>\n",
       "      <td>1.137117</td>\n",
       "      <td>12.745852</td>\n",
       "      <td>16.629165</td>\n",
       "      <td>6.272014</td>\n",
       "      <td>3.177633</td>\n",
       "      <td>8.931124</td>\n",
       "      <td>12.155618</td>\n",
       "      <td>-11.946744</td>\n",
       "      <td>0.874170</td>\n",
       "      <td>0.661173</td>\n",
       "      <td>6.369157</td>\n",
       "      <td>0.982891</td>\n",
       "      <td>5.794039</td>\n",
       "      <td>11.943223</td>\n",
       "      <td>5.018893</td>\n",
       "      <td>-3.331515</td>\n",
       "      <td>24.446811</td>\n",
       "      <td>0.669756</td>\n",
       "      <td>0.640553</td>\n",
       "      <td>19.610888</td>\n",
       "      <td>19.518846</td>\n",
       "      <td>16.853732</td>\n",
       "      <td>6.050871</td>\n",
       "      <td>19.066993</td>\n",
       "      <td>5.349479</td>\n",
       "      <td>14.402136</td>\n",
       "      <td>5.795044</td>\n",
       "      <td>14.719024</td>\n",
       "      <td>-3.471273</td>\n",
       "      <td>1.025817</td>\n",
       "      <td>-2.590209</td>\n",
       "      <td>18.362721</td>\n",
       "      <td>5.621058</td>\n",
       "      <td>11.351483</td>\n",
       "      <td>8.702924</td>\n",
       "      <td>3.725208</td>\n",
       "      <td>-16.548147</td>\n",
       "      <td>6.987541</td>\n",
       "      <td>12.739578</td>\n",
       "      <td>10.556740</td>\n",
       "      <td>10.999162</td>\n",
       "      <td>-0.084344</td>\n",
       "      <td>14.400433</td>\n",
       "      <td>18.539645</td>\n",
       "      <td>1.752012</td>\n",
       "      <td>-0.746296</td>\n",
       "      <td>-6.600518</td>\n",
       "      <td>13.413526</td>\n",
       "      <td>22.294908</td>\n",
       "      <td>1.568393</td>\n",
       "      <td>11.509834</td>\n",
       "      <td>4.244744</td>\n",
       "      <td>8.617657</td>\n",
       "      <td>17.796266</td>\n",
       "      <td>14.224435</td>\n",
       "      <td>18.458001</td>\n",
       "      <td>5.513238</td>\n",
       "      <td>6.312603</td>\n",
       "      <td>3.317843</td>\n",
       "      <td>8.136542</td>\n",
       "      <td>3.081191</td>\n",
       "      <td>2.213717</td>\n",
       "      <td>2.402570</td>\n",
       "      <td>16.102233</td>\n",
       "      <td>-5.305132</td>\n",
       "      <td>3.032849</td>\n",
       "      <td>24.521078</td>\n",
       "      <td>11.310591</td>\n",
       "      <td>1.192984</td>\n",
       "      <td>7.076254</td>\n",
       "      <td>4.272740</td>\n",
       "      <td>12.489165</td>\n",
       "      <td>13.202326</td>\n",
       "      <td>0.851507</td>\n",
       "      <td>-1.127952</td>\n",
       "      <td>15.460314</td>\n",
       "      <td>12.257151</td>\n",
       "      <td>0.544674</td>\n",
       "      <td>7.799676</td>\n",
       "      <td>6.813270</td>\n",
       "      <td>-4.826053</td>\n",
       "      <td>-4.259472</td>\n",
       "      <td>22.968602</td>\n",
       "      <td>17.613651</td>\n",
       "      <td>1.210792</td>\n",
       "      <td>7.760193</td>\n",
       "      <td>3.423636</td>\n",
       "      <td>2.897596</td>\n",
       "      <td>11.983489</td>\n",
       "      <td>12.333698</td>\n",
       "      <td>8.647632</td>\n",
       "      <td>4.841328</td>\n",
       "      <td>10.341178</td>\n",
       "      <td>-3.300779</td>\n",
       "      <td>3.990726</td>\n",
       "      <td>5.296237</td>\n",
       "      <td>16.817671</td>\n",
       "      <td>10.141542</td>\n",
       "      <td>7.633199</td>\n",
       "      <td>16.727902</td>\n",
       "      <td>6.974955</td>\n",
       "      <td>-2.074128</td>\n",
       "      <td>13.209272</td>\n",
       "      <td>-4.813552</td>\n",
       "      <td>17.914591</td>\n",
       "      <td>10.223282</td>\n",
       "      <td>24.259300</td>\n",
       "      <td>5.633293</td>\n",
       "      <td>5.362896</td>\n",
       "      <td>11.002170</td>\n",
       "      <td>-2.871906</td>\n",
       "      <td>19.315753</td>\n",
       "      <td>2.963335</td>\n",
       "      <td>-4.151155</td>\n",
       "      <td>4.937124</td>\n",
       "      <td>5.636008</td>\n",
       "      <td>-0.004962</td>\n",
       "      <td>-0.831777</td>\n",
       "      <td>19.817094</td>\n",
       "      <td>-0.677967</td>\n",
       "      <td>20.210677</td>\n",
       "      <td>11.640613</td>\n",
       "      <td>-2.799585</td>\n",
       "      <td>11.882933</td>\n",
       "      <td>-1.014064</td>\n",
       "      <td>2.591444</td>\n",
       "      <td>-2.741666</td>\n",
       "      <td>10.085518</td>\n",
       "      <td>0.719109</td>\n",
       "      <td>8.769088</td>\n",
       "      <td>12.756676</td>\n",
       "      <td>-3.983261</td>\n",
       "      <td>8.970274</td>\n",
       "      <td>-10.335043</td>\n",
       "      <td>15.377174</td>\n",
       "      <td>0.746072</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>1.235070</td>\n",
       "      <td>5.500793</td>\n",
       "      <td>5.970253</td>\n",
       "      <td>0.190059</td>\n",
       "      <td>4.639536</td>\n",
       "      <td>2.247908</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>2.557421</td>\n",
       "      <td>6.712612</td>\n",
       "      <td>7.851370</td>\n",
       "      <td>7.996694</td>\n",
       "      <td>5.876254</td>\n",
       "      <td>8.196564</td>\n",
       "      <td>2.847958</td>\n",
       "      <td>0.526893</td>\n",
       "      <td>3.777245</td>\n",
       "      <td>0.285535</td>\n",
       "      <td>5.922210</td>\n",
       "      <td>1.523714</td>\n",
       "      <td>0.783367</td>\n",
       "      <td>2.615942</td>\n",
       "      <td>7.965198</td>\n",
       "      <td>2.159891</td>\n",
       "      <td>2.587830</td>\n",
       "      <td>4.322325</td>\n",
       "      <td>0.541614</td>\n",
       "      <td>5.179559</td>\n",
       "      <td>3.119978</td>\n",
       "      <td>2.249730</td>\n",
       "      <td>4.278903</td>\n",
       "      <td>4.068845</td>\n",
       "      <td>8.279259</td>\n",
       "      <td>5.938088</td>\n",
       "      <td>0.695991</td>\n",
       "      <td>0.309599</td>\n",
       "      <td>5.903073</td>\n",
       "      <td>21.404912</td>\n",
       "      <td>2.860511</td>\n",
       "      <td>10.579862</td>\n",
       "      <td>11.384332</td>\n",
       "      <td>7.855762</td>\n",
       "      <td>0.691709</td>\n",
       "      <td>8.187306</td>\n",
       "      <td>4.985532</td>\n",
       "      <td>0.764753</td>\n",
       "      <td>8.414241</td>\n",
       "      <td>5.690072</td>\n",
       "      <td>3.540174</td>\n",
       "      <td>0.795026</td>\n",
       "      <td>4.296686</td>\n",
       "      <td>0.854798</td>\n",
       "      <td>4.222389</td>\n",
       "      <td>11.622948</td>\n",
       "      <td>2.026238</td>\n",
       "      <td>3.113089</td>\n",
       "      <td>1.485854</td>\n",
       "      <td>3.786493</td>\n",
       "      <td>1.121366</td>\n",
       "      <td>7.365115</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>3.955723</td>\n",
       "      <td>11.951742</td>\n",
       "      <td>0.266696</td>\n",
       "      <td>3.944703</td>\n",
       "      <td>7.466303</td>\n",
       "      <td>14.112591</td>\n",
       "      <td>6.055322</td>\n",
       "      <td>7.938351</td>\n",
       "      <td>3.817292</td>\n",
       "      <td>1.993792</td>\n",
       "      <td>1.309055</td>\n",
       "      <td>7.436737</td>\n",
       "      <td>2.299567</td>\n",
       "      <td>8.479255</td>\n",
       "      <td>8.297229</td>\n",
       "      <td>6.225305</td>\n",
       "      <td>3.908536</td>\n",
       "      <td>7.751142</td>\n",
       "      <td>5.661867</td>\n",
       "      <td>2.491460</td>\n",
       "      <td>3.560554</td>\n",
       "      <td>13.152810</td>\n",
       "      <td>0.152641</td>\n",
       "      <td>4.186252</td>\n",
       "      <td>0.543341</td>\n",
       "      <td>2.768099</td>\n",
       "      <td>0.621125</td>\n",
       "      <td>8.525400</td>\n",
       "      <td>12.642382</td>\n",
       "      <td>0.715836</td>\n",
       "      <td>1.862550</td>\n",
       "      <td>9.181683</td>\n",
       "      <td>4.950537</td>\n",
       "      <td>8.628179</td>\n",
       "      <td>0.185020</td>\n",
       "      <td>1.970520</td>\n",
       "      <td>0.855698</td>\n",
       "      <td>1.894899</td>\n",
       "      <td>7.604723</td>\n",
       "      <td>0.171091</td>\n",
       "      <td>4.355031</td>\n",
       "      <td>3.823253</td>\n",
       "      <td>1.082404</td>\n",
       "      <td>1.591170</td>\n",
       "      <td>4.459077</td>\n",
       "      <td>0.985396</td>\n",
       "      <td>2.621851</td>\n",
       "      <td>1.650912</td>\n",
       "      <td>13.297662</td>\n",
       "      <td>8.799268</td>\n",
       "      <td>4.182796</td>\n",
       "      <td>12.121016</td>\n",
       "      <td>1.714416</td>\n",
       "      <td>5.168479</td>\n",
       "      <td>6.147345</td>\n",
       "      <td>2.736821</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.776056</td>\n",
       "      <td>3.137684</td>\n",
       "      <td>3.238043</td>\n",
       "      <td>4.136453</td>\n",
       "      <td>0.832199</td>\n",
       "      <td>0.456280</td>\n",
       "      <td>1.456486</td>\n",
       "      <td>0.375603</td>\n",
       "      <td>6.166126</td>\n",
       "      <td>7.617732</td>\n",
       "      <td>10.382235</td>\n",
       "      <td>8.890516</td>\n",
       "      <td>4.551750</td>\n",
       "      <td>7.686433</td>\n",
       "      <td>4.896325</td>\n",
       "      <td>6.715637</td>\n",
       "      <td>5.691936</td>\n",
       "      <td>2.934706</td>\n",
       "      <td>0.922469</td>\n",
       "      <td>3.899281</td>\n",
       "      <td>2.518883</td>\n",
       "      <td>7.413301</td>\n",
       "      <td>0.199192</td>\n",
       "      <td>10.385133</td>\n",
       "      <td>2.464157</td>\n",
       "      <td>3.962426</td>\n",
       "      <td>3.005373</td>\n",
       "      <td>2.014200</td>\n",
       "      <td>4.961678</td>\n",
       "      <td>5.771261</td>\n",
       "      <td>0.955140</td>\n",
       "      <td>5.570272</td>\n",
       "      <td>7.885579</td>\n",
       "      <td>4.122912</td>\n",
       "      <td>10.880263</td>\n",
       "      <td>0.217938</td>\n",
       "      <td>1.419612</td>\n",
       "      <td>5.262056</td>\n",
       "      <td>5.457784</td>\n",
       "      <td>5.024182</td>\n",
       "      <td>0.369684</td>\n",
       "      <td>7.798020</td>\n",
       "      <td>3.105986</td>\n",
       "      <td>0.369437</td>\n",
       "      <td>4.424621</td>\n",
       "      <td>5.378008</td>\n",
       "      <td>8.674171</td>\n",
       "      <td>5.966674</td>\n",
       "      <td>7.136427</td>\n",
       "      <td>2.892167</td>\n",
       "      <td>7.513939</td>\n",
       "      <td>2.628895</td>\n",
       "      <td>8.579810</td>\n",
       "      <td>2.798956</td>\n",
       "      <td>5.261243</td>\n",
       "      <td>1.371862</td>\n",
       "      <td>8.963434</td>\n",
       "      <td>4.474924</td>\n",
       "      <td>9.318280</td>\n",
       "      <td>4.725167</td>\n",
       "      <td>3.189759</td>\n",
       "      <td>11.574708</td>\n",
       "      <td>3.944604</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>-20.731300</td>\n",
       "      <td>-26.095000</td>\n",
       "      <td>13.434600</td>\n",
       "      <td>-6.011100</td>\n",
       "      <td>1.013300</td>\n",
       "      <td>13.076900</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>-33.380200</td>\n",
       "      <td>-10.664200</td>\n",
       "      <td>-12.402500</td>\n",
       "      <td>-5.432200</td>\n",
       "      <td>-10.089000</td>\n",
       "      <td>-5.322500</td>\n",
       "      <td>1.209800</td>\n",
       "      <td>-0.678400</td>\n",
       "      <td>12.720000</td>\n",
       "      <td>-24.243100</td>\n",
       "      <td>-6.166800</td>\n",
       "      <td>2.089600</td>\n",
       "      <td>-4.787200</td>\n",
       "      <td>-34.798400</td>\n",
       "      <td>2.140600</td>\n",
       "      <td>-8.986100</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>9.816900</td>\n",
       "      <td>-16.513600</td>\n",
       "      <td>-8.095100</td>\n",
       "      <td>-1.183400</td>\n",
       "      <td>-6.337100</td>\n",
       "      <td>-14.545700</td>\n",
       "      <td>-35.211700</td>\n",
       "      <td>-8.535900</td>\n",
       "      <td>8.859000</td>\n",
       "      <td>10.652800</td>\n",
       "      <td>-9.939600</td>\n",
       "      <td>-90.252500</td>\n",
       "      <td>1.206200</td>\n",
       "      <td>-47.686200</td>\n",
       "      <td>-23.902200</td>\n",
       "      <td>-8.070700</td>\n",
       "      <td>10.385500</td>\n",
       "      <td>-15.046200</td>\n",
       "      <td>-24.721400</td>\n",
       "      <td>3.344900</td>\n",
       "      <td>-26.778600</td>\n",
       "      <td>-3.782600</td>\n",
       "      <td>2.761800</td>\n",
       "      <td>3.442300</td>\n",
       "      <td>-12.600900</td>\n",
       "      <td>6.184000</td>\n",
       "      <td>-2.100600</td>\n",
       "      <td>-48.802700</td>\n",
       "      <td>-6.328900</td>\n",
       "      <td>-10.554400</td>\n",
       "      <td>1.611700</td>\n",
       "      <td>-14.088800</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>-19.544300</td>\n",
       "      <td>4.993800</td>\n",
       "      <td>-16.309400</td>\n",
       "      <td>-17.027500</td>\n",
       "      <td>-0.224000</td>\n",
       "      <td>-12.383400</td>\n",
       "      <td>-1.665800</td>\n",
       "      <td>-34.101500</td>\n",
       "      <td>-1.293600</td>\n",
       "      <td>-21.633300</td>\n",
       "      <td>7.425700</td>\n",
       "      <td>-1.818300</td>\n",
       "      <td>10.445400</td>\n",
       "      <td>-18.042200</td>\n",
       "      <td>7.586500</td>\n",
       "      <td>-30.026600</td>\n",
       "      <td>-24.220100</td>\n",
       "      <td>-24.439800</td>\n",
       "      <td>7.023000</td>\n",
       "      <td>-19.272200</td>\n",
       "      <td>-8.481600</td>\n",
       "      <td>1.350200</td>\n",
       "      <td>-9.601400</td>\n",
       "      <td>-61.718000</td>\n",
       "      <td>6.521800</td>\n",
       "      <td>-1.018500</td>\n",
       "      <td>8.491600</td>\n",
       "      <td>2.819000</td>\n",
       "      <td>-2.432400</td>\n",
       "      <td>-12.158400</td>\n",
       "      <td>-21.740000</td>\n",
       "      <td>-0.603500</td>\n",
       "      <td>-7.280600</td>\n",
       "      <td>-39.179100</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>-7.382900</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>4.084600</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>-5.898000</td>\n",
       "      <td>13.729000</td>\n",
       "      <td>5.769700</td>\n",
       "      <td>-9.239800</td>\n",
       "      <td>2.194200</td>\n",
       "      <td>-2.030200</td>\n",
       "      <td>-5.513900</td>\n",
       "      <td>-0.050500</td>\n",
       "      <td>-6.858600</td>\n",
       "      <td>-3.163000</td>\n",
       "      <td>-31.836900</td>\n",
       "      <td>-37.527700</td>\n",
       "      <td>-9.774200</td>\n",
       "      <td>-18.696200</td>\n",
       "      <td>6.305200</td>\n",
       "      <td>-15.194000</td>\n",
       "      <td>-12.405900</td>\n",
       "      <td>-7.053800</td>\n",
       "      <td>11.486100</td>\n",
       "      <td>11.265400</td>\n",
       "      <td>-8.876900</td>\n",
       "      <td>-11.755900</td>\n",
       "      <td>2.186300</td>\n",
       "      <td>9.528300</td>\n",
       "      <td>-0.954800</td>\n",
       "      <td>2.890000</td>\n",
       "      <td>5.359300</td>\n",
       "      <td>-24.254600</td>\n",
       "      <td>-31.380800</td>\n",
       "      <td>-9.949300</td>\n",
       "      <td>-9.851000</td>\n",
       "      <td>-16.468400</td>\n",
       "      <td>-21.274300</td>\n",
       "      <td>-15.459500</td>\n",
       "      <td>-16.693700</td>\n",
       "      <td>-7.108000</td>\n",
       "      <td>2.806800</td>\n",
       "      <td>5.444300</td>\n",
       "      <td>-8.273400</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>-29.984000</td>\n",
       "      <td>3.320500</td>\n",
       "      <td>-41.168300</td>\n",
       "      <td>9.242000</td>\n",
       "      <td>-2.191500</td>\n",
       "      <td>-2.880000</td>\n",
       "      <td>11.030800</td>\n",
       "      <td>-8.196600</td>\n",
       "      <td>-21.840900</td>\n",
       "      <td>9.996500</td>\n",
       "      <td>-22.990400</td>\n",
       "      <td>-4.554400</td>\n",
       "      <td>-4.641600</td>\n",
       "      <td>-7.452200</td>\n",
       "      <td>4.852600</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>-6.531700</td>\n",
       "      <td>-19.997700</td>\n",
       "      <td>3.816700</td>\n",
       "      <td>1.851200</td>\n",
       "      <td>-35.969500</td>\n",
       "      <td>-5.250200</td>\n",
       "      <td>4.258800</td>\n",
       "      <td>-14.506000</td>\n",
       "      <td>-22.479300</td>\n",
       "      <td>-11.453300</td>\n",
       "      <td>-22.748700</td>\n",
       "      <td>-2.995300</td>\n",
       "      <td>3.241500</td>\n",
       "      <td>-29.116500</td>\n",
       "      <td>4.952100</td>\n",
       "      <td>-29.273400</td>\n",
       "      <td>-7.856100</td>\n",
       "      <td>-22.037400</td>\n",
       "      <td>5.416500</td>\n",
       "      <td>-26.001100</td>\n",
       "      <td>-4.808200</td>\n",
       "      <td>-18.489700</td>\n",
       "      <td>-22.583300</td>\n",
       "      <td>-3.022300</td>\n",
       "      <td>-47.753600</td>\n",
       "      <td>4.412300</td>\n",
       "      <td>-2.554300</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>6.618800</td>\n",
       "      <td>-3.594950</td>\n",
       "      <td>-7.510600</td>\n",
       "      <td>13.894000</td>\n",
       "      <td>5.072800</td>\n",
       "      <td>5.781875</td>\n",
       "      <td>14.262800</td>\n",
       "      <td>7.452275</td>\n",
       "      <td>-10.476225</td>\n",
       "      <td>9.177950</td>\n",
       "      <td>6.276475</td>\n",
       "      <td>8.627800</td>\n",
       "      <td>11.551000</td>\n",
       "      <td>2.182400</td>\n",
       "      <td>2.634100</td>\n",
       "      <td>7.613000</td>\n",
       "      <td>13.456400</td>\n",
       "      <td>-8.321725</td>\n",
       "      <td>-2.307900</td>\n",
       "      <td>4.992100</td>\n",
       "      <td>3.171700</td>\n",
       "      <td>-13.766175</td>\n",
       "      <td>8.870000</td>\n",
       "      <td>-2.500875</td>\n",
       "      <td>11.456300</td>\n",
       "      <td>11.032300</td>\n",
       "      <td>0.116975</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>4.125475</td>\n",
       "      <td>7.591050</td>\n",
       "      <td>-2.199500</td>\n",
       "      <td>-12.831825</td>\n",
       "      <td>4.519575</td>\n",
       "      <td>10.713200</td>\n",
       "      <td>11.343800</td>\n",
       "      <td>5.313650</td>\n",
       "      <td>-28.730700</td>\n",
       "      <td>9.248750</td>\n",
       "      <td>-20.654525</td>\n",
       "      <td>6.351975</td>\n",
       "      <td>10.653475</td>\n",
       "      <td>12.269000</td>\n",
       "      <td>7.267625</td>\n",
       "      <td>-6.065025</td>\n",
       "      <td>5.435600</td>\n",
       "      <td>-5.147625</td>\n",
       "      <td>8.163900</td>\n",
       "      <td>14.097875</td>\n",
       "      <td>5.687500</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>8.312400</td>\n",
       "      <td>8.912750</td>\n",
       "      <td>-20.901725</td>\n",
       "      <td>-0.572400</td>\n",
       "      <td>-1.588700</td>\n",
       "      <td>5.293500</td>\n",
       "      <td>-1.702800</td>\n",
       "      <td>4.973800</td>\n",
       "      <td>6.753200</td>\n",
       "      <td>5.014000</td>\n",
       "      <td>-6.336625</td>\n",
       "      <td>15.256625</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>-2.197100</td>\n",
       "      <td>14.097275</td>\n",
       "      <td>9.595975</td>\n",
       "      <td>12.480975</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>16.014700</td>\n",
       "      <td>3.817275</td>\n",
       "      <td>13.375400</td>\n",
       "      <td>0.694475</td>\n",
       "      <td>13.214775</td>\n",
       "      <td>-10.004950</td>\n",
       "      <td>-5.106400</td>\n",
       "      <td>-7.216125</td>\n",
       "      <td>15.338575</td>\n",
       "      <td>0.407550</td>\n",
       "      <td>7.247175</td>\n",
       "      <td>6.918775</td>\n",
       "      <td>1.140500</td>\n",
       "      <td>-26.665600</td>\n",
       "      <td>6.869900</td>\n",
       "      <td>9.670300</td>\n",
       "      <td>10.195600</td>\n",
       "      <td>8.828000</td>\n",
       "      <td>-0.527400</td>\n",
       "      <td>7.796950</td>\n",
       "      <td>8.919525</td>\n",
       "      <td>1.267675</td>\n",
       "      <td>-2.106200</td>\n",
       "      <td>-13.198700</td>\n",
       "      <td>9.639800</td>\n",
       "      <td>16.047975</td>\n",
       "      <td>1.428900</td>\n",
       "      <td>10.097900</td>\n",
       "      <td>3.639600</td>\n",
       "      <td>7.282300</td>\n",
       "      <td>12.168075</td>\n",
       "      <td>14.098900</td>\n",
       "      <td>15.107175</td>\n",
       "      <td>2.817475</td>\n",
       "      <td>5.510100</td>\n",
       "      <td>2.092675</td>\n",
       "      <td>4.803250</td>\n",
       "      <td>2.388775</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>6.373500</td>\n",
       "      <td>-11.587850</td>\n",
       "      <td>-0.161975</td>\n",
       "      <td>15.696275</td>\n",
       "      <td>9.996400</td>\n",
       "      <td>-2.565200</td>\n",
       "      <td>2.817050</td>\n",
       "      <td>2.353600</td>\n",
       "      <td>12.245400</td>\n",
       "      <td>12.608400</td>\n",
       "      <td>-1.502325</td>\n",
       "      <td>-3.580725</td>\n",
       "      <td>12.514475</td>\n",
       "      <td>11.619300</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>6.724375</td>\n",
       "      <td>6.543500</td>\n",
       "      <td>-9.625700</td>\n",
       "      <td>-9.957100</td>\n",
       "      <td>14.933900</td>\n",
       "      <td>10.656550</td>\n",
       "      <td>-2.011825</td>\n",
       "      <td>2.387575</td>\n",
       "      <td>-0.121700</td>\n",
       "      <td>-2.153725</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>10.311200</td>\n",
       "      <td>7.968075</td>\n",
       "      <td>1.885875</td>\n",
       "      <td>8.646900</td>\n",
       "      <td>-8.751450</td>\n",
       "      <td>3.853600</td>\n",
       "      <td>-1.903200</td>\n",
       "      <td>14.952200</td>\n",
       "      <td>7.064600</td>\n",
       "      <td>5.567900</td>\n",
       "      <td>15.233000</td>\n",
       "      <td>3.339900</td>\n",
       "      <td>-6.266025</td>\n",
       "      <td>12.475100</td>\n",
       "      <td>-8.939950</td>\n",
       "      <td>12.109200</td>\n",
       "      <td>7.243525</td>\n",
       "      <td>15.696125</td>\n",
       "      <td>5.470500</td>\n",
       "      <td>4.326100</td>\n",
       "      <td>7.029600</td>\n",
       "      <td>-7.094025</td>\n",
       "      <td>15.744550</td>\n",
       "      <td>2.699000</td>\n",
       "      <td>-9.643100</td>\n",
       "      <td>2.703200</td>\n",
       "      <td>5.374600</td>\n",
       "      <td>-3.258500</td>\n",
       "      <td>-4.720350</td>\n",
       "      <td>13.731775</td>\n",
       "      <td>-5.009525</td>\n",
       "      <td>15.064600</td>\n",
       "      <td>9.371600</td>\n",
       "      <td>-8.386500</td>\n",
       "      <td>9.808675</td>\n",
       "      <td>-7.395700</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>-6.673900</td>\n",
       "      <td>9.084700</td>\n",
       "      <td>-6.064425</td>\n",
       "      <td>5.423100</td>\n",
       "      <td>5.663300</td>\n",
       "      <td>-7.360000</td>\n",
       "      <td>6.715200</td>\n",
       "      <td>-19.205125</td>\n",
       "      <td>12.501550</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>7.629600</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>-3.286950</td>\n",
       "      <td>14.025500</td>\n",
       "      <td>8.604250</td>\n",
       "      <td>7.520300</td>\n",
       "      <td>14.574100</td>\n",
       "      <td>9.232050</td>\n",
       "      <td>-5.666350</td>\n",
       "      <td>15.196250</td>\n",
       "      <td>12.453900</td>\n",
       "      <td>13.196800</td>\n",
       "      <td>17.234250</td>\n",
       "      <td>4.275150</td>\n",
       "      <td>3.008650</td>\n",
       "      <td>10.380350</td>\n",
       "      <td>13.662500</td>\n",
       "      <td>-4.196900</td>\n",
       "      <td>-1.132100</td>\n",
       "      <td>5.534850</td>\n",
       "      <td>4.950200</td>\n",
       "      <td>-7.411750</td>\n",
       "      <td>10.365650</td>\n",
       "      <td>-0.497650</td>\n",
       "      <td>14.576000</td>\n",
       "      <td>11.435200</td>\n",
       "      <td>3.917750</td>\n",
       "      <td>2.198000</td>\n",
       "      <td>5.900650</td>\n",
       "      <td>10.562700</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>-6.617450</td>\n",
       "      <td>9.162650</td>\n",
       "      <td>11.243400</td>\n",
       "      <td>11.565000</td>\n",
       "      <td>9.437200</td>\n",
       "      <td>-12.547200</td>\n",
       "      <td>11.310750</td>\n",
       "      <td>-12.482400</td>\n",
       "      <td>14.559200</td>\n",
       "      <td>16.672400</td>\n",
       "      <td>12.745600</td>\n",
       "      <td>13.444400</td>\n",
       "      <td>-2.502450</td>\n",
       "      <td>6.027800</td>\n",
       "      <td>1.274050</td>\n",
       "      <td>12.594100</td>\n",
       "      <td>16.648150</td>\n",
       "      <td>6.262500</td>\n",
       "      <td>3.170100</td>\n",
       "      <td>8.901000</td>\n",
       "      <td>12.064350</td>\n",
       "      <td>-11.892000</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>6.377700</td>\n",
       "      <td>1.021350</td>\n",
       "      <td>5.782000</td>\n",
       "      <td>11.922000</td>\n",
       "      <td>5.019100</td>\n",
       "      <td>-3.325500</td>\n",
       "      <td>24.445000</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.646450</td>\n",
       "      <td>19.309750</td>\n",
       "      <td>19.536650</td>\n",
       "      <td>16.844200</td>\n",
       "      <td>6.297800</td>\n",
       "      <td>18.967850</td>\n",
       "      <td>5.440050</td>\n",
       "      <td>14.388850</td>\n",
       "      <td>6.061750</td>\n",
       "      <td>14.844500</td>\n",
       "      <td>-3.284450</td>\n",
       "      <td>1.069700</td>\n",
       "      <td>-2.517950</td>\n",
       "      <td>18.296450</td>\n",
       "      <td>6.006700</td>\n",
       "      <td>11.288000</td>\n",
       "      <td>8.616200</td>\n",
       "      <td>3.642550</td>\n",
       "      <td>-16.482600</td>\n",
       "      <td>6.986500</td>\n",
       "      <td>12.673500</td>\n",
       "      <td>10.582200</td>\n",
       "      <td>10.983850</td>\n",
       "      <td>-0.098600</td>\n",
       "      <td>14.369900</td>\n",
       "      <td>18.502150</td>\n",
       "      <td>1.768300</td>\n",
       "      <td>-0.771300</td>\n",
       "      <td>-6.401500</td>\n",
       "      <td>13.380850</td>\n",
       "      <td>22.306850</td>\n",
       "      <td>1.566000</td>\n",
       "      <td>11.497950</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>8.605150</td>\n",
       "      <td>17.573200</td>\n",
       "      <td>14.226600</td>\n",
       "      <td>18.281350</td>\n",
       "      <td>5.394300</td>\n",
       "      <td>6.340100</td>\n",
       "      <td>3.408400</td>\n",
       "      <td>8.148550</td>\n",
       "      <td>3.083800</td>\n",
       "      <td>2.249850</td>\n",
       "      <td>2.456300</td>\n",
       "      <td>15.944850</td>\n",
       "      <td>-5.189500</td>\n",
       "      <td>3.023950</td>\n",
       "      <td>24.354700</td>\n",
       "      <td>11.239700</td>\n",
       "      <td>1.200700</td>\n",
       "      <td>7.234300</td>\n",
       "      <td>4.302100</td>\n",
       "      <td>12.486300</td>\n",
       "      <td>13.166800</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>-1.101750</td>\n",
       "      <td>15.426800</td>\n",
       "      <td>12.264650</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>7.809100</td>\n",
       "      <td>6.806700</td>\n",
       "      <td>-4.704250</td>\n",
       "      <td>-4.111900</td>\n",
       "      <td>22.948300</td>\n",
       "      <td>17.257250</td>\n",
       "      <td>1.211750</td>\n",
       "      <td>8.066250</td>\n",
       "      <td>3.564700</td>\n",
       "      <td>2.975500</td>\n",
       "      <td>11.855900</td>\n",
       "      <td>12.356350</td>\n",
       "      <td>8.651850</td>\n",
       "      <td>4.904700</td>\n",
       "      <td>10.395600</td>\n",
       "      <td>-3.178700</td>\n",
       "      <td>3.996000</td>\n",
       "      <td>5.283250</td>\n",
       "      <td>16.736950</td>\n",
       "      <td>10.127900</td>\n",
       "      <td>7.673700</td>\n",
       "      <td>16.649750</td>\n",
       "      <td>6.994050</td>\n",
       "      <td>-2.066100</td>\n",
       "      <td>13.184300</td>\n",
       "      <td>-4.868400</td>\n",
       "      <td>17.630450</td>\n",
       "      <td>10.217550</td>\n",
       "      <td>23.864500</td>\n",
       "      <td>5.633500</td>\n",
       "      <td>5.359700</td>\n",
       "      <td>10.788700</td>\n",
       "      <td>-2.637800</td>\n",
       "      <td>19.270800</td>\n",
       "      <td>2.960200</td>\n",
       "      <td>-4.011600</td>\n",
       "      <td>4.761600</td>\n",
       "      <td>5.634300</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-0.807350</td>\n",
       "      <td>19.748000</td>\n",
       "      <td>-0.569750</td>\n",
       "      <td>20.206100</td>\n",
       "      <td>11.679800</td>\n",
       "      <td>-2.538450</td>\n",
       "      <td>11.737250</td>\n",
       "      <td>-0.942050</td>\n",
       "      <td>2.512300</td>\n",
       "      <td>-2.688800</td>\n",
       "      <td>10.036050</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>12.521000</td>\n",
       "      <td>-3.946950</td>\n",
       "      <td>8.902150</td>\n",
       "      <td>-10.209750</td>\n",
       "      <td>15.239450</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>8.584425</td>\n",
       "      <td>4.382925</td>\n",
       "      <td>0.852825</td>\n",
       "      <td>14.164200</td>\n",
       "      <td>12.274775</td>\n",
       "      <td>9.270425</td>\n",
       "      <td>14.874500</td>\n",
       "      <td>11.055900</td>\n",
       "      <td>-0.810775</td>\n",
       "      <td>21.013325</td>\n",
       "      <td>18.433300</td>\n",
       "      <td>17.879400</td>\n",
       "      <td>23.089050</td>\n",
       "      <td>6.293200</td>\n",
       "      <td>3.403800</td>\n",
       "      <td>13.479600</td>\n",
       "      <td>13.863700</td>\n",
       "      <td>-0.090200</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>6.093700</td>\n",
       "      <td>6.798925</td>\n",
       "      <td>-1.443450</td>\n",
       "      <td>11.885000</td>\n",
       "      <td>1.469100</td>\n",
       "      <td>18.097125</td>\n",
       "      <td>11.844400</td>\n",
       "      <td>7.487725</td>\n",
       "      <td>4.460400</td>\n",
       "      <td>7.542400</td>\n",
       "      <td>13.598925</td>\n",
       "      <td>3.637825</td>\n",
       "      <td>-0.880875</td>\n",
       "      <td>13.754800</td>\n",
       "      <td>11.756900</td>\n",
       "      <td>11.804600</td>\n",
       "      <td>13.087300</td>\n",
       "      <td>3.150525</td>\n",
       "      <td>13.318300</td>\n",
       "      <td>-4.244525</td>\n",
       "      <td>23.028650</td>\n",
       "      <td>22.549050</td>\n",
       "      <td>13.234500</td>\n",
       "      <td>19.385650</td>\n",
       "      <td>0.944350</td>\n",
       "      <td>6.542900</td>\n",
       "      <td>7.401825</td>\n",
       "      <td>17.086625</td>\n",
       "      <td>19.289700</td>\n",
       "      <td>6.845000</td>\n",
       "      <td>6.209700</td>\n",
       "      <td>9.566525</td>\n",
       "      <td>15.116500</td>\n",
       "      <td>-3.225450</td>\n",
       "      <td>2.228200</td>\n",
       "      <td>3.020300</td>\n",
       "      <td>7.490600</td>\n",
       "      <td>3.739200</td>\n",
       "      <td>6.586200</td>\n",
       "      <td>17.037650</td>\n",
       "      <td>5.024100</td>\n",
       "      <td>-0.498875</td>\n",
       "      <td>33.633150</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>3.510700</td>\n",
       "      <td>25.207125</td>\n",
       "      <td>29.620700</td>\n",
       "      <td>21.432225</td>\n",
       "      <td>11.818800</td>\n",
       "      <td>22.041100</td>\n",
       "      <td>6.867200</td>\n",
       "      <td>15.383100</td>\n",
       "      <td>11.449125</td>\n",
       "      <td>16.340800</td>\n",
       "      <td>3.101725</td>\n",
       "      <td>7.449900</td>\n",
       "      <td>1.986700</td>\n",
       "      <td>21.358850</td>\n",
       "      <td>11.158375</td>\n",
       "      <td>15.433225</td>\n",
       "      <td>10.567025</td>\n",
       "      <td>6.146200</td>\n",
       "      <td>-6.409375</td>\n",
       "      <td>7.101400</td>\n",
       "      <td>15.840225</td>\n",
       "      <td>10.944900</td>\n",
       "      <td>13.089100</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>20.819375</td>\n",
       "      <td>28.158975</td>\n",
       "      <td>2.260900</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>17.250225</td>\n",
       "      <td>28.682225</td>\n",
       "      <td>1.705400</td>\n",
       "      <td>12.902100</td>\n",
       "      <td>4.822200</td>\n",
       "      <td>9.928900</td>\n",
       "      <td>23.348600</td>\n",
       "      <td>14.361800</td>\n",
       "      <td>21.852900</td>\n",
       "      <td>8.104325</td>\n",
       "      <td>7.080300</td>\n",
       "      <td>4.577400</td>\n",
       "      <td>11.596200</td>\n",
       "      <td>3.811900</td>\n",
       "      <td>4.121500</td>\n",
       "      <td>3.665100</td>\n",
       "      <td>25.780825</td>\n",
       "      <td>0.971800</td>\n",
       "      <td>6.098400</td>\n",
       "      <td>33.105275</td>\n",
       "      <td>12.619425</td>\n",
       "      <td>5.091700</td>\n",
       "      <td>11.734750</td>\n",
       "      <td>6.192200</td>\n",
       "      <td>12.718100</td>\n",
       "      <td>13.811700</td>\n",
       "      <td>3.293000</td>\n",
       "      <td>1.351700</td>\n",
       "      <td>18.480400</td>\n",
       "      <td>12.876700</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>8.911425</td>\n",
       "      <td>7.070800</td>\n",
       "      <td>-0.178800</td>\n",
       "      <td>1.125950</td>\n",
       "      <td>31.042425</td>\n",
       "      <td>24.426025</td>\n",
       "      <td>4.391225</td>\n",
       "      <td>13.232525</td>\n",
       "      <td>7.078525</td>\n",
       "      <td>8.192425</td>\n",
       "      <td>16.073925</td>\n",
       "      <td>14.461050</td>\n",
       "      <td>9.315000</td>\n",
       "      <td>7.676925</td>\n",
       "      <td>12.113225</td>\n",
       "      <td>2.028275</td>\n",
       "      <td>4.131600</td>\n",
       "      <td>12.688225</td>\n",
       "      <td>18.682500</td>\n",
       "      <td>13.057600</td>\n",
       "      <td>9.817300</td>\n",
       "      <td>18.263900</td>\n",
       "      <td>10.766350</td>\n",
       "      <td>1.891750</td>\n",
       "      <td>13.929300</td>\n",
       "      <td>-0.988575</td>\n",
       "      <td>23.875325</td>\n",
       "      <td>13.094525</td>\n",
       "      <td>32.622850</td>\n",
       "      <td>5.792000</td>\n",
       "      <td>6.371200</td>\n",
       "      <td>14.623900</td>\n",
       "      <td>1.323600</td>\n",
       "      <td>23.024025</td>\n",
       "      <td>3.241500</td>\n",
       "      <td>1.318725</td>\n",
       "      <td>7.020025</td>\n",
       "      <td>5.905400</td>\n",
       "      <td>3.096400</td>\n",
       "      <td>2.956800</td>\n",
       "      <td>25.907725</td>\n",
       "      <td>3.619900</td>\n",
       "      <td>25.641225</td>\n",
       "      <td>13.745500</td>\n",
       "      <td>2.704400</td>\n",
       "      <td>13.931300</td>\n",
       "      <td>5.338750</td>\n",
       "      <td>4.391125</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>11.011300</td>\n",
       "      <td>7.499175</td>\n",
       "      <td>12.127425</td>\n",
       "      <td>19.456150</td>\n",
       "      <td>-0.590650</td>\n",
       "      <td>11.193800</td>\n",
       "      <td>-1.466000</td>\n",
       "      <td>18.345225</td>\n",
       "      <td>1.482900</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>11.150600</td>\n",
       "      <td>18.670200</td>\n",
       "      <td>17.188700</td>\n",
       "      <td>14.654500</td>\n",
       "      <td>22.331500</td>\n",
       "      <td>14.937700</td>\n",
       "      <td>15.863300</td>\n",
       "      <td>17.950600</td>\n",
       "      <td>19.025900</td>\n",
       "      <td>41.748000</td>\n",
       "      <td>35.183000</td>\n",
       "      <td>31.285900</td>\n",
       "      <td>49.044300</td>\n",
       "      <td>14.594500</td>\n",
       "      <td>4.875200</td>\n",
       "      <td>25.446000</td>\n",
       "      <td>14.654600</td>\n",
       "      <td>15.675100</td>\n",
       "      <td>3.243100</td>\n",
       "      <td>8.787400</td>\n",
       "      <td>13.143100</td>\n",
       "      <td>15.651500</td>\n",
       "      <td>20.171900</td>\n",
       "      <td>6.787100</td>\n",
       "      <td>29.546600</td>\n",
       "      <td>13.287800</td>\n",
       "      <td>21.528900</td>\n",
       "      <td>14.245600</td>\n",
       "      <td>11.863800</td>\n",
       "      <td>29.823500</td>\n",
       "      <td>15.322300</td>\n",
       "      <td>18.105600</td>\n",
       "      <td>26.165800</td>\n",
       "      <td>13.469600</td>\n",
       "      <td>12.577900</td>\n",
       "      <td>34.196100</td>\n",
       "      <td>62.084400</td>\n",
       "      <td>21.293900</td>\n",
       "      <td>20.685400</td>\n",
       "      <td>54.273800</td>\n",
       "      <td>41.153000</td>\n",
       "      <td>15.317200</td>\n",
       "      <td>40.689000</td>\n",
       "      <td>17.096800</td>\n",
       "      <td>8.231500</td>\n",
       "      <td>28.572400</td>\n",
       "      <td>29.092100</td>\n",
       "      <td>29.074100</td>\n",
       "      <td>9.160900</td>\n",
       "      <td>20.483300</td>\n",
       "      <td>11.986700</td>\n",
       "      <td>25.195500</td>\n",
       "      <td>27.102900</td>\n",
       "      <td>7.753600</td>\n",
       "      <td>11.231700</td>\n",
       "      <td>11.153700</td>\n",
       "      <td>15.731300</td>\n",
       "      <td>9.713200</td>\n",
       "      <td>39.396800</td>\n",
       "      <td>5.046900</td>\n",
       "      <td>8.547300</td>\n",
       "      <td>64.464400</td>\n",
       "      <td>1.571900</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>44.536100</td>\n",
       "      <td>70.272000</td>\n",
       "      <td>36.156700</td>\n",
       "      <td>34.435200</td>\n",
       "      <td>30.956900</td>\n",
       "      <td>11.350700</td>\n",
       "      <td>18.225600</td>\n",
       "      <td>30.476900</td>\n",
       "      <td>23.132400</td>\n",
       "      <td>21.893400</td>\n",
       "      <td>27.714300</td>\n",
       "      <td>17.742400</td>\n",
       "      <td>32.901100</td>\n",
       "      <td>34.563700</td>\n",
       "      <td>33.354100</td>\n",
       "      <td>17.459400</td>\n",
       "      <td>15.481600</td>\n",
       "      <td>27.271300</td>\n",
       "      <td>7.489500</td>\n",
       "      <td>26.997600</td>\n",
       "      <td>12.534300</td>\n",
       "      <td>18.975000</td>\n",
       "      <td>1.804000</td>\n",
       "      <td>40.880600</td>\n",
       "      <td>58.287900</td>\n",
       "      <td>4.502800</td>\n",
       "      <td>5.076400</td>\n",
       "      <td>25.140900</td>\n",
       "      <td>28.459400</td>\n",
       "      <td>51.326500</td>\n",
       "      <td>2.188700</td>\n",
       "      <td>19.020600</td>\n",
       "      <td>7.169200</td>\n",
       "      <td>15.307400</td>\n",
       "      <td>46.379500</td>\n",
       "      <td>14.743000</td>\n",
       "      <td>32.059100</td>\n",
       "      <td>19.519300</td>\n",
       "      <td>9.800200</td>\n",
       "      <td>8.431700</td>\n",
       "      <td>21.542100</td>\n",
       "      <td>6.585000</td>\n",
       "      <td>11.950400</td>\n",
       "      <td>8.120700</td>\n",
       "      <td>64.810900</td>\n",
       "      <td>25.263500</td>\n",
       "      <td>15.688500</td>\n",
       "      <td>74.032100</td>\n",
       "      <td>17.307400</td>\n",
       "      <td>18.471400</td>\n",
       "      <td>26.874900</td>\n",
       "      <td>14.991500</td>\n",
       "      <td>13.664200</td>\n",
       "      <td>15.515600</td>\n",
       "      <td>10.597600</td>\n",
       "      <td>9.809600</td>\n",
       "      <td>31.203600</td>\n",
       "      <td>14.989500</td>\n",
       "      <td>2.192300</td>\n",
       "      <td>12.465000</td>\n",
       "      <td>8.309100</td>\n",
       "      <td>12.723600</td>\n",
       "      <td>21.412800</td>\n",
       "      <td>54.579400</td>\n",
       "      <td>44.437600</td>\n",
       "      <td>18.818700</td>\n",
       "      <td>36.097100</td>\n",
       "      <td>21.121900</td>\n",
       "      <td>23.965800</td>\n",
       "      <td>32.891100</td>\n",
       "      <td>22.691600</td>\n",
       "      <td>11.810100</td>\n",
       "      <td>16.008300</td>\n",
       "      <td>20.437300</td>\n",
       "      <td>22.149400</td>\n",
       "      <td>4.752800</td>\n",
       "      <td>48.424000</td>\n",
       "      <td>25.435700</td>\n",
       "      <td>21.124500</td>\n",
       "      <td>18.384600</td>\n",
       "      <td>24.007500</td>\n",
       "      <td>23.242800</td>\n",
       "      <td>16.831600</td>\n",
       "      <td>16.497000</td>\n",
       "      <td>11.972100</td>\n",
       "      <td>44.779500</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>58.394200</td>\n",
       "      <td>6.309900</td>\n",
       "      <td>10.134400</td>\n",
       "      <td>27.564800</td>\n",
       "      <td>12.119300</td>\n",
       "      <td>38.332200</td>\n",
       "      <td>4.220400</td>\n",
       "      <td>21.276600</td>\n",
       "      <td>14.886100</td>\n",
       "      <td>7.089000</td>\n",
       "      <td>16.731900</td>\n",
       "      <td>17.917300</td>\n",
       "      <td>53.591900</td>\n",
       "      <td>18.855400</td>\n",
       "      <td>43.546800</td>\n",
       "      <td>20.854800</td>\n",
       "      <td>20.245200</td>\n",
       "      <td>20.596500</td>\n",
       "      <td>29.841300</td>\n",
       "      <td>13.448700</td>\n",
       "      <td>12.750500</td>\n",
       "      <td>14.393900</td>\n",
       "      <td>29.248700</td>\n",
       "      <td>23.704900</td>\n",
       "      <td>44.363400</td>\n",
       "      <td>12.997500</td>\n",
       "      <td>21.739200</td>\n",
       "      <td>22.786100</td>\n",
       "      <td>29.330300</td>\n",
       "      <td>4.034100</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          var_0          var_1          var_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.100490      10.679914      -1.627622      10.715192   \n",
       "std         0.300653       3.040051       4.050044       2.640894   \n",
       "min         0.000000       0.408400     -15.043400       2.117100   \n",
       "25%         0.000000       8.453850      -4.740025       8.722475   \n",
       "50%         0.000000      10.524750      -1.608050      10.580000   \n",
       "75%         0.000000      12.758200       1.358625      12.516700   \n",
       "max         1.000000      20.315000      10.376800      19.353000   \n",
       "\n",
       "               var_3          var_4          var_5          var_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.796529      11.078333      -5.065317       5.408949   \n",
       "std         2.043319       1.623150       7.863267       0.866607   \n",
       "min        -0.040200       5.074800     -32.562600       2.347300   \n",
       "25%         5.254075       9.883175     -11.200350       4.767700   \n",
       "50%         6.825000      11.108250      -4.833150       5.385100   \n",
       "75%         8.324100      12.261125       0.924800       6.003000   \n",
       "max        13.188300      16.671400      17.251600       8.447700   \n",
       "\n",
       "               var_7          var_8          var_9         var_10  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       16.545850       0.284162       7.567236       0.394340   \n",
       "std         3.418076       3.332634       1.235070       5.500793   \n",
       "min         5.349700     -10.505500       3.970500     -20.731300   \n",
       "25%        13.943800      -2.317800       6.618800      -3.594950   \n",
       "50%        16.456800       0.393700       7.629600       0.487300   \n",
       "75%        19.102900       2.937900       8.584425       4.382925   \n",
       "max        27.691800      10.151300      11.150600      18.670200   \n",
       "\n",
       "              var_11         var_12         var_13         var_14  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -3.245596      14.023978       8.530232       7.537606   \n",
       "std         5.970253       0.190059       4.639536       2.247908   \n",
       "min       -26.095000      13.434600      -6.011100       1.013300   \n",
       "25%        -7.510600      13.894000       5.072800       5.781875   \n",
       "50%        -3.286950      14.025500       8.604250       7.520300   \n",
       "75%         0.852825      14.164200      12.274775       9.270425   \n",
       "max        17.188700      14.654500      22.331500      14.937700   \n",
       "\n",
       "              var_15         var_16         var_17         var_18  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       14.573126       9.333264      -5.696731      15.244013   \n",
       "std         0.411711       2.557421       6.712612       7.851370   \n",
       "min        13.076900       0.635100     -33.380200     -10.664200   \n",
       "25%        14.262800       7.452275     -10.476225       9.177950   \n",
       "50%        14.574100       9.232050      -5.666350      15.196250   \n",
       "75%        14.874500      11.055900      -0.810775      21.013325   \n",
       "max        15.863300      17.950600      19.025900      41.748000   \n",
       "\n",
       "              var_19         var_20         var_21         var_22  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       12.438567      13.290894      17.257883       4.305430   \n",
       "std         7.996694       5.876254       8.196564       2.847958   \n",
       "min       -12.402500      -5.432200     -10.089000      -5.322500   \n",
       "25%         6.276475       8.627800      11.551000       2.182400   \n",
       "50%        12.453900      13.196800      17.234250       4.275150   \n",
       "75%        18.433300      17.879400      23.089050       6.293200   \n",
       "max        35.183000      31.285900      49.044300      14.594500   \n",
       "\n",
       "              var_23         var_24         var_25         var_26  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        3.019540      10.584400      13.667496      -4.055133   \n",
       "std         0.526893       3.777245       0.285535       5.922210   \n",
       "min         1.209800      -0.678400      12.720000     -24.243100   \n",
       "25%         2.634100       7.613000      13.456400      -8.321725   \n",
       "50%         3.008650      10.380350      13.662500      -4.196900   \n",
       "75%         3.403800      13.479600      13.863700      -0.090200   \n",
       "max         4.875200      25.446000      14.654600      15.675100   \n",
       "\n",
       "              var_27         var_28         var_29         var_30  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -1.137908       5.532980       5.053874      -7.687740   \n",
       "std         1.523714       0.783367       2.615942       7.965198   \n",
       "min        -6.166800       2.089600      -4.787200     -34.798400   \n",
       "25%        -2.307900       4.992100       3.171700     -13.766175   \n",
       "50%        -1.132100       5.534850       4.950200      -7.411750   \n",
       "75%         0.015625       6.093700       6.798925      -1.443450   \n",
       "max         3.243100       8.787400      13.143100      15.651500   \n",
       "\n",
       "              var_31         var_32         var_33         var_34  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       10.393046      -0.512886      14.774147      11.434250   \n",
       "std         2.159891       2.587830       4.322325       0.541614   \n",
       "min         2.140600      -8.986100       1.508500       9.816900   \n",
       "25%         8.870000      -2.500875      11.456300      11.032300   \n",
       "50%        10.365650      -0.497650      14.576000      11.435200   \n",
       "75%        11.885000       1.469100      18.097125      11.844400   \n",
       "max        20.171900       6.787100      29.546600      13.287800   \n",
       "\n",
       "              var_35         var_36         var_37         var_38  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        3.842499       2.187230       5.868899      10.642131   \n",
       "std         5.179559       3.119978       2.249730       4.278903   \n",
       "min       -16.513600      -8.095100      -1.183400      -6.337100   \n",
       "25%         0.116975      -0.007125       4.125475       7.591050   \n",
       "50%         3.917750       2.198000       5.900650      10.562700   \n",
       "75%         7.487725       4.460400       7.542400      13.598925   \n",
       "max        21.528900      14.245600      11.863800      29.823500   \n",
       "\n",
       "              var_39         var_40         var_41         var_42  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.662956      -6.725505       9.299858      11.222356   \n",
       "std         4.068845       8.279259       5.938088       0.695991   \n",
       "min       -14.545700     -35.211700      -8.535900       8.859000   \n",
       "25%        -2.199500     -12.831825       4.519575      10.713200   \n",
       "50%         0.672300      -6.617450       9.162650      11.243400   \n",
       "75%         3.637825      -0.880875      13.754800      11.756900   \n",
       "max        15.322300      18.105600      26.165800      13.469600   \n",
       "\n",
       "              var_43         var_44         var_45         var_46  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.569954       8.948289     -12.699667      11.326488   \n",
       "std         0.309599       5.903073      21.404912       2.860511   \n",
       "min        10.652800      -9.939600     -90.252500       1.206200   \n",
       "25%        11.343800       5.313650     -28.730700       9.248750   \n",
       "50%        11.565000       9.437200     -12.547200      11.310750   \n",
       "75%        11.804600      13.087300       3.150525      13.318300   \n",
       "max        12.577900      34.196100      62.084400      21.293900   \n",
       "\n",
       "              var_47         var_48         var_49         var_50  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean      -12.471737      14.704713      16.682499      12.740986   \n",
       "std        10.579862      11.384332       7.855762       0.691709   \n",
       "min       -47.686200     -23.902200      -8.070700      10.385500   \n",
       "25%       -20.654525       6.351975      10.653475      12.269000   \n",
       "50%       -12.482400      14.559200      16.672400      12.745600   \n",
       "75%        -4.244525      23.028650      22.549050      13.234500   \n",
       "max        20.685400      54.273800      41.153000      15.317200   \n",
       "\n",
       "              var_51         var_52         var_53         var_54  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       13.428912      -2.528816       6.008569       1.137117   \n",
       "std         8.187306       4.985532       0.764753       8.414241   \n",
       "min       -15.046200     -24.721400       3.344900     -26.778600   \n",
       "25%         7.267625      -6.065025       5.435600      -5.147625   \n",
       "50%        13.444400      -2.502450       6.027800       1.274050   \n",
       "75%        19.385650       0.944350       6.542900       7.401825   \n",
       "max        40.689000      17.096800       8.231500      28.572400   \n",
       "\n",
       "              var_55         var_56         var_57         var_58  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       12.745852      16.629165       6.272014       3.177633   \n",
       "std         5.690072       3.540174       0.795026       4.296686   \n",
       "min        -3.782600       2.761800       3.442300     -12.600900   \n",
       "25%         8.163900      14.097875       5.687500       0.183500   \n",
       "50%        12.594100      16.648150       6.262500       3.170100   \n",
       "75%        17.086625      19.289700       6.845000       6.209700   \n",
       "max        29.092100      29.074100       9.160900      20.483300   \n",
       "\n",
       "              var_59         var_60         var_61         var_62  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        8.931124      12.155618     -11.946744       0.874170   \n",
       "std         0.854798       4.222389      11.622948       2.026238   \n",
       "min         6.184000      -2.100600     -48.802700      -6.328900   \n",
       "25%         8.312400       8.912750     -20.901725      -0.572400   \n",
       "50%         8.901000      12.064350     -11.892000       0.794700   \n",
       "75%         9.566525      15.116500      -3.225450       2.228200   \n",
       "max        11.986700      25.195500      27.102900       7.753600   \n",
       "\n",
       "              var_63         var_64         var_65         var_66  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.661173       6.369157       0.982891       5.794039   \n",
       "std         3.113089       1.485854       3.786493       1.121366   \n",
       "min       -10.554400       1.611700     -14.088800       1.336800   \n",
       "25%        -1.588700       5.293500      -1.702800       4.973800   \n",
       "50%         0.681700       6.377700       1.021350       5.782000   \n",
       "75%         3.020300       7.490600       3.739200       6.586200   \n",
       "max        11.231700      11.153700      15.731300       9.713200   \n",
       "\n",
       "              var_67         var_68         var_69         var_70  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.943223       5.018893      -3.331515      24.446811   \n",
       "std         7.365115       0.007186       3.955723      11.951742   \n",
       "min       -19.544300       4.993800     -16.309400     -17.027500   \n",
       "25%         6.753200       5.014000      -6.336625      15.256625   \n",
       "50%        11.922000       5.019100      -3.325500      24.445000   \n",
       "75%        17.037650       5.024100      -0.498875      33.633150   \n",
       "max        39.396800       5.046900       8.547300      64.464400   \n",
       "\n",
       "              var_71         var_72         var_73         var_74  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.669756       0.640553      19.610888      19.518846   \n",
       "std         0.266696       3.944703       7.466303      14.112591   \n",
       "min        -0.224000     -12.383400      -1.665800     -34.101500   \n",
       "25%         0.472300      -2.197100      14.097275       9.595975   \n",
       "50%         0.668400       0.646450      19.309750      19.536650   \n",
       "75%         0.864400       3.510700      25.207125      29.620700   \n",
       "max         1.571900      14.150000      44.536100      70.272000   \n",
       "\n",
       "              var_75         var_76         var_77         var_78  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       16.853732       6.050871      19.066993       5.349479   \n",
       "std         6.055322       7.938351       3.817292       1.993792   \n",
       "min        -1.293600     -21.633300       7.425700      -1.818300   \n",
       "25%        12.480975       0.596300      16.014700       3.817275   \n",
       "50%        16.844200       6.297800      18.967850       5.440050   \n",
       "75%        21.432225      11.818800      22.041100       6.867200   \n",
       "max        36.156700      34.435200      30.956900      11.350700   \n",
       "\n",
       "              var_79         var_80         var_81         var_82  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       14.402136       5.795044      14.719024      -3.471273   \n",
       "std         1.309055       7.436737       2.299567       8.479255   \n",
       "min        10.445400     -18.042200       7.586500     -30.026600   \n",
       "25%        13.375400       0.694475      13.214775     -10.004950   \n",
       "50%        14.388850       6.061750      14.844500      -3.284450   \n",
       "75%        15.383100      11.449125      16.340800       3.101725   \n",
       "max        18.225600      30.476900      23.132400      21.893400   \n",
       "\n",
       "              var_83         var_84         var_85         var_86  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.025817      -2.590209      18.362721       5.621058   \n",
       "std         8.297229       6.225305       3.908536       7.751142   \n",
       "min       -24.220100     -24.439800       7.023000     -19.272200   \n",
       "25%        -5.106400      -7.216125      15.338575       0.407550   \n",
       "50%         1.069700      -2.517950      18.296450       6.006700   \n",
       "75%         7.449900       1.986700      21.358850      11.158375   \n",
       "max        27.714300      17.742400      32.901100      34.563700   \n",
       "\n",
       "              var_87         var_88         var_89         var_90  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.351483       8.702924       3.725208     -16.548147   \n",
       "std         5.661867       2.491460       3.560554      13.152810   \n",
       "min        -8.481600       1.350200      -9.601400     -61.718000   \n",
       "25%         7.247175       6.918775       1.140500     -26.665600   \n",
       "50%        11.288000       8.616200       3.642550     -16.482600   \n",
       "75%        15.433225      10.567025       6.146200      -6.409375   \n",
       "max        33.354100      17.459400      15.481600      27.271300   \n",
       "\n",
       "              var_91         var_92         var_93         var_94  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.987541      12.739578      10.556740      10.999162   \n",
       "std         0.152641       4.186252       0.543341       2.768099   \n",
       "min         6.521800      -1.018500       8.491600       2.819000   \n",
       "25%         6.869900       9.670300      10.195600       8.828000   \n",
       "50%         6.986500      12.673500      10.582200      10.983850   \n",
       "75%         7.101400      15.840225      10.944900      13.089100   \n",
       "max         7.489500      26.997600      12.534300      18.975000   \n",
       "\n",
       "              var_95         var_96         var_97         var_98  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.084344      14.400433      18.539645       1.752012   \n",
       "std         0.621125       8.525400      12.642382       0.715836   \n",
       "min        -2.432400     -12.158400     -21.740000      -0.603500   \n",
       "25%        -0.527400       7.796950       8.919525       1.267675   \n",
       "50%        -0.098600      14.369900      18.502150       1.768300   \n",
       "75%         0.329100      20.819375      28.158975       2.260900   \n",
       "max         1.804000      40.880600      58.287900       4.502800   \n",
       "\n",
       "              var_99        var_100        var_101        var_102  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.746296      -6.600518      13.413526      22.294908   \n",
       "std         1.862550       9.181683       4.950537       8.628179   \n",
       "min        -7.280600     -39.179100       0.075700      -7.382900   \n",
       "25%        -2.106200     -13.198700       9.639800      16.047975   \n",
       "50%        -0.771300      -6.401500      13.380850      22.306850   \n",
       "75%         0.528500       0.132100      17.250225      28.682225   \n",
       "max         5.076400      25.140900      28.459400      51.326500   \n",
       "\n",
       "             var_103        var_104        var_105        var_106  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.568393      11.509834       4.244744       8.617657   \n",
       "std         0.185020       1.970520       0.855698       1.894899   \n",
       "min         0.979300       4.084600       0.715300       0.942400   \n",
       "25%         1.428900      10.097900       3.639600       7.282300   \n",
       "50%         1.566000      11.497950       4.224500       8.605150   \n",
       "75%         1.705400      12.902100       4.822200       9.928900   \n",
       "max         2.188700      19.020600       7.169200      15.307400   \n",
       "\n",
       "             var_107        var_108        var_109        var_110  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       17.796266      14.224435      18.458001       5.513238   \n",
       "std         7.604723       0.171091       4.355031       3.823253   \n",
       "min        -5.898000      13.729000       5.769700      -9.239800   \n",
       "25%        12.168075      14.098900      15.107175       2.817475   \n",
       "50%        17.573200      14.226600      18.281350       5.394300   \n",
       "75%        23.348600      14.361800      21.852900       8.104325   \n",
       "max        46.379500      14.743000      32.059100      19.519300   \n",
       "\n",
       "             var_111        var_112        var_113        var_114  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.312603       3.317843       8.136542       3.081191   \n",
       "std         1.082404       1.591170       4.459077       0.985396   \n",
       "min         2.194200      -2.030200      -5.513900      -0.050500   \n",
       "25%         5.510100       2.092675       4.803250       2.388775   \n",
       "50%         6.340100       3.408400       8.148550       3.083800   \n",
       "75%         7.080300       4.577400      11.596200       3.811900   \n",
       "max         9.800200       8.431700      21.542100       6.585000   \n",
       "\n",
       "             var_115        var_116        var_117        var_118  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        2.213717       2.402570      16.102233      -5.305132   \n",
       "std         2.621851       1.650912      13.297662       8.799268   \n",
       "min        -6.858600      -3.163000     -31.836900     -37.527700   \n",
       "25%         0.399700       1.171875       6.373500     -11.587850   \n",
       "50%         2.249850       2.456300      15.944850      -5.189500   \n",
       "75%         4.121500       3.665100      25.780825       0.971800   \n",
       "max        11.950400       8.120700      64.810900      25.263500   \n",
       "\n",
       "             var_119        var_120        var_121        var_122  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        3.032849      24.521078      11.310591       1.192984   \n",
       "std         4.182796      12.121016       1.714416       5.168479   \n",
       "min        -9.774200     -18.696200       6.305200     -15.194000   \n",
       "25%        -0.161975      15.696275       9.996400      -2.565200   \n",
       "50%         3.023950      24.354700      11.239700       1.200700   \n",
       "75%         6.098400      33.105275      12.619425       5.091700   \n",
       "max        15.688500      74.032100      17.307400      18.471400   \n",
       "\n",
       "             var_123        var_124        var_125        var_126  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.076254       4.272740      12.489165      13.202326   \n",
       "std         6.147345       2.736821       0.318100       0.776056   \n",
       "min       -12.405900      -7.053800      11.486100      11.265400   \n",
       "25%         2.817050       2.353600      12.245400      12.608400   \n",
       "50%         7.234300       4.302100      12.486300      13.166800   \n",
       "75%        11.734750       6.192200      12.718100      13.811700   \n",
       "max        26.874900      14.991500      13.664200      15.515600   \n",
       "\n",
       "             var_127        var_128        var_129        var_130  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.851507      -1.127952      15.460314      12.257151   \n",
       "std         3.137684       3.238043       4.136453       0.832199   \n",
       "min        -8.876900     -11.755900       2.186300       9.528300   \n",
       "25%        -1.502325      -3.580725      12.514475      11.619300   \n",
       "50%         0.925000      -1.101750      15.426800      12.264650   \n",
       "75%         3.293000       1.351700      18.480400      12.876700   \n",
       "max        10.597600       9.809600      31.203600      14.989500   \n",
       "\n",
       "             var_131        var_132        var_133        var_134  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.544674       7.799676       6.813270      -4.826053   \n",
       "std         0.456280       1.456486       0.375603       6.166126   \n",
       "min        -0.954800       2.890000       5.359300     -24.254600   \n",
       "25%         0.207800       6.724375       6.543500      -9.625700   \n",
       "50%         0.556600       7.809100       6.806700      -4.704250   \n",
       "75%         0.901000       8.911425       7.070800      -0.178800   \n",
       "max         2.192300      12.465000       8.309100      12.723600   \n",
       "\n",
       "             var_135        var_136        var_137        var_138  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -4.259472      22.968602      17.613651       1.210792   \n",
       "std         7.617732      10.382235       8.890516       4.551750   \n",
       "min       -31.380800      -9.949300      -9.851000     -16.468400   \n",
       "25%        -9.957100      14.933900      10.656550      -2.011825   \n",
       "50%        -4.111900      22.948300      17.257250       1.211750   \n",
       "75%         1.125950      31.042425      24.426025       4.391225   \n",
       "max        21.412800      54.579400      44.437600      18.818700   \n",
       "\n",
       "             var_139        var_140        var_141        var_142  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.760193       3.423636       2.897596      11.983489   \n",
       "std         7.686433       4.896325       6.715637       5.691936   \n",
       "min       -21.274300     -15.459500     -16.693700      -7.108000   \n",
       "25%         2.387575      -0.121700      -2.153725       7.900000   \n",
       "50%         8.066250       3.564700       2.975500      11.855900   \n",
       "75%        13.232525       7.078525       8.192425      16.073925   \n",
       "max        36.097100      21.121900      23.965800      32.891100   \n",
       "\n",
       "             var_143        var_144        var_145        var_146  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       12.333698       8.647632       4.841328      10.341178   \n",
       "std         2.934706       0.922469       3.899281       2.518883   \n",
       "min         2.806800       5.444300      -8.273400       0.427400   \n",
       "25%        10.311200       7.968075       1.885875       8.646900   \n",
       "50%        12.356350       8.651850       4.904700      10.395600   \n",
       "75%        14.461050       9.315000       7.676925      12.113225   \n",
       "max        22.691600      11.810100      16.008300      20.437300   \n",
       "\n",
       "             var_147        var_148        var_149        var_150  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -3.300779       3.990726       5.296237      16.817671   \n",
       "std         7.413301       0.199192      10.385133       2.464157   \n",
       "min       -29.984000       3.320500     -41.168300       9.242000   \n",
       "25%        -8.751450       3.853600      -1.903200      14.952200   \n",
       "50%        -3.178700       3.996000       5.283250      16.736950   \n",
       "75%         2.028275       4.131600      12.688225      18.682500   \n",
       "max        22.149400       4.752800      48.424000      25.435700   \n",
       "\n",
       "             var_151        var_152        var_153        var_154  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       10.141542       7.633199      16.727902       6.974955   \n",
       "std         3.962426       3.005373       2.014200       4.961678   \n",
       "min        -2.191500      -2.880000      11.030800      -8.196600   \n",
       "25%         7.064600       5.567900      15.233000       3.339900   \n",
       "50%        10.127900       7.673700      16.649750       6.994050   \n",
       "75%        13.057600       9.817300      18.263900      10.766350   \n",
       "max        21.124500      18.384600      24.007500      23.242800   \n",
       "\n",
       "             var_155        var_156        var_157        var_158  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -2.074128      13.209272      -4.813552      17.914591   \n",
       "std         5.771261       0.955140       5.570272       7.885579   \n",
       "min       -21.840900       9.996500     -22.990400      -4.554400   \n",
       "25%        -6.266025      12.475100      -8.939950      12.109200   \n",
       "50%        -2.066100      13.184300      -4.868400      17.630450   \n",
       "75%         1.891750      13.929300      -0.988575      23.875325   \n",
       "max        16.831600      16.497000      11.972100      44.779500   \n",
       "\n",
       "             var_159        var_160        var_161        var_162  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       10.223282      24.259300       5.633293       5.362896   \n",
       "std         4.122912      10.880263       0.217938       1.419612   \n",
       "min        -4.641600      -7.452200       4.852600       0.623100   \n",
       "25%         7.243525      15.696125       5.470500       4.326100   \n",
       "50%        10.217550      23.864500       5.633500       5.359700   \n",
       "75%        13.094525      32.622850       5.792000       6.371200   \n",
       "max        25.120000      58.394200       6.309900      10.134400   \n",
       "\n",
       "             var_163        var_164        var_165        var_166  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.002170      -2.871906      19.315753       2.963335   \n",
       "std         5.262056       5.457784       5.024182       0.369684   \n",
       "min        -6.531700     -19.997700       3.816700       1.851200   \n",
       "25%         7.029600      -7.094025      15.744550       2.699000   \n",
       "50%        10.788700      -2.637800      19.270800       2.960200   \n",
       "75%        14.623900       1.323600      23.024025       3.241500   \n",
       "max        27.564800      12.119300      38.332200       4.220400   \n",
       "\n",
       "             var_167        var_168        var_169        var_170  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -4.151155       4.937124       5.636008      -0.004962   \n",
       "std         7.798020       3.105986       0.369437       4.424621   \n",
       "min       -35.969500      -5.250200       4.258800     -14.506000   \n",
       "25%        -9.643100       2.703200       5.374600      -3.258500   \n",
       "50%        -4.011600       4.761600       5.634300       0.002800   \n",
       "75%         1.318725       7.020025       5.905400       3.096400   \n",
       "max        21.276600      14.886100       7.089000      16.731900   \n",
       "\n",
       "             var_171        var_172        var_173        var_174  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.831777      19.817094      -0.677967      20.210677   \n",
       "std         5.378008       8.674171       5.966674       7.136427   \n",
       "min       -22.479300     -11.453300     -22.748700      -2.995300   \n",
       "25%        -4.720350      13.731775      -5.009525      15.064600   \n",
       "50%        -0.807350      19.748000      -0.569750      20.206100   \n",
       "75%         2.956800      25.907725       3.619900      25.641225   \n",
       "max        17.917300      53.591900      18.855400      43.546800   \n",
       "\n",
       "             var_175        var_176        var_177        var_178  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.640613      -2.799585      11.882933      -1.014064   \n",
       "std         2.892167       7.513939       2.628895       8.579810   \n",
       "min         3.241500     -29.116500       4.952100     -29.273400   \n",
       "25%         9.371600      -8.386500       9.808675      -7.395700   \n",
       "50%        11.679800      -2.538450      11.737250      -0.942050   \n",
       "75%        13.745500       2.704400      13.931300       5.338750   \n",
       "max        20.854800      20.245200      20.596500      29.841300   \n",
       "\n",
       "             var_179        var_180        var_181        var_182  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        2.591444      -2.741666      10.085518       0.719109   \n",
       "std         2.798956       5.261243       1.371862       8.963434   \n",
       "min        -7.856100     -22.037400       5.416500     -26.001100   \n",
       "25%         0.625575      -6.673900       9.084700      -6.064425   \n",
       "50%         2.512300      -2.688800      10.036050       0.720200   \n",
       "75%         4.391125       0.996200      11.011300       7.499175   \n",
       "max        13.448700      12.750500      14.393900      29.248700   \n",
       "\n",
       "             var_183        var_184        var_185        var_186  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        8.769088      12.756676      -3.983261       8.970274   \n",
       "std         4.474924       9.318280       4.725167       3.189759   \n",
       "min        -4.808200     -18.489700     -22.583300      -3.022300   \n",
       "25%         5.423100       5.663300      -7.360000       6.715200   \n",
       "50%         8.600000      12.521000      -3.946950       8.902150   \n",
       "75%        12.127425      19.456150      -0.590650      11.193800   \n",
       "max        23.704900      44.363400      12.997500      21.739200   \n",
       "\n",
       "             var_187        var_188        var_189        var_190  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean      -10.335043      15.377174       0.746072       3.234440   \n",
       "std        11.574708       3.944604       0.976348       4.559922   \n",
       "min       -47.753600       4.412300      -2.554300     -14.093300   \n",
       "25%       -19.205125      12.501550       0.014900      -0.058825   \n",
       "50%       -10.209750      15.239450       0.742600       3.203600   \n",
       "75%        -1.466000      18.345225       1.482900       6.406200   \n",
       "max        22.786100      29.330300       4.034100      18.440900   \n",
       "\n",
       "             var_191        var_192        var_193        var_194  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.438408       1.927839       3.331774      17.993784   \n",
       "std         3.023272       1.478423       3.992030       3.135162   \n",
       "min        -2.691700      -3.814500     -11.783400       8.694400   \n",
       "25%         5.157400       0.889775       0.584600      15.629800   \n",
       "50%         7.347750       1.901300       3.396350      17.957950   \n",
       "75%         9.512525       2.949500       6.205800      20.396525   \n",
       "max        16.716500       8.402400      18.281800      27.928800   \n",
       "\n",
       "             var_195        var_196        var_197        var_198  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.142088       2.303335       8.908158      15.870720   \n",
       "std         1.429372       5.454369       0.921625       3.010945   \n",
       "min        -5.261000     -14.209600       5.960600       6.299300   \n",
       "25%        -1.170700      -1.946925       8.252800      13.829700   \n",
       "50%        -0.172700       2.408900       8.888200      15.934050   \n",
       "75%         0.829600       6.556725       9.593300      18.064725   \n",
       "max         4.272900      18.321500      12.000400      26.079100   \n",
       "\n",
       "             var_199  \n",
       "count  200000.000000  \n",
       "mean       -3.326537  \n",
       "std        10.438015  \n",
       "min       -38.852800  \n",
       "25%       -11.208475  \n",
       "50%        -2.819550  \n",
       "75%         4.836800  \n",
       "max        28.500700  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()\n",
    "# visual inspection suggests data is cleaned and no outliers; could do hist plots as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = df_train.isna().sum()\n",
    "missing[missing>0]\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 200), (40000, 200), (160000,), (40000,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_columns = df_train.columns[2:]\n",
    "X = df_train.loc[:, feat_columns]\n",
    "y = df_train.loc[:, 'target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Classifier & Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 200) (40000,)\n"
     ]
    }
   ],
   "source": [
    "model3 = xgb.XGBClassifier(learning_rate=0.1,\n",
    "                           max_depth=5,                 \n",
    "                           n_estimators=100,        # max # of trees; Number of boosting rounds\n",
    "                           subsample=0.5,           # Subsample ratio of the training instanc\n",
    "                           colsample_bytree=0.5,    # Subsample ratio of columns when constructing each tree\n",
    "                           eval_metric='auc',       # Evaluation metrics for validation data - 'auc', 'error', 'logloss' etc.\n",
    "                           verbosity=1,\n",
    "                           use_label_encoder=False)\n",
    "\n",
    "test_set = [(X_test, y_test)]                       # note \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.63145\n",
      "[1]\tvalidation_0-auc:0.65483\n",
      "[2]\tvalidation_0-auc:0.67125\n",
      "[3]\tvalidation_0-auc:0.68133\n",
      "[4]\tvalidation_0-auc:0.69606\n",
      "[5]\tvalidation_0-auc:0.69658\n",
      "[6]\tvalidation_0-auc:0.69683\n",
      "[7]\tvalidation_0-auc:0.70017\n",
      "[8]\tvalidation_0-auc:0.70263\n",
      "[9]\tvalidation_0-auc:0.70939\n",
      "[10]\tvalidation_0-auc:0.71253\n",
      "[11]\tvalidation_0-auc:0.72071\n",
      "[12]\tvalidation_0-auc:0.72464\n",
      "[13]\tvalidation_0-auc:0.72687\n",
      "[14]\tvalidation_0-auc:0.73383\n",
      "[15]\tvalidation_0-auc:0.73625\n",
      "[16]\tvalidation_0-auc:0.74156\n",
      "[17]\tvalidation_0-auc:0.74391\n",
      "[18]\tvalidation_0-auc:0.74736\n",
      "[19]\tvalidation_0-auc:0.75296\n",
      "[20]\tvalidation_0-auc:0.75505\n",
      "[21]\tvalidation_0-auc:0.75928\n",
      "[22]\tvalidation_0-auc:0.76035\n",
      "[23]\tvalidation_0-auc:0.76316\n",
      "[24]\tvalidation_0-auc:0.76561\n",
      "[25]\tvalidation_0-auc:0.77000\n",
      "[26]\tvalidation_0-auc:0.77415\n",
      "[27]\tvalidation_0-auc:0.77494\n",
      "[28]\tvalidation_0-auc:0.77717\n",
      "[29]\tvalidation_0-auc:0.77898\n",
      "[30]\tvalidation_0-auc:0.78136\n",
      "[31]\tvalidation_0-auc:0.78339\n",
      "[32]\tvalidation_0-auc:0.78531\n",
      "[33]\tvalidation_0-auc:0.78830\n",
      "[34]\tvalidation_0-auc:0.79148\n",
      "[35]\tvalidation_0-auc:0.79301\n",
      "[36]\tvalidation_0-auc:0.79372\n",
      "[37]\tvalidation_0-auc:0.79473\n",
      "[38]\tvalidation_0-auc:0.79560\n",
      "[39]\tvalidation_0-auc:0.79754\n",
      "[40]\tvalidation_0-auc:0.79951\n",
      "[41]\tvalidation_0-auc:0.80193\n",
      "[42]\tvalidation_0-auc:0.80395\n",
      "[43]\tvalidation_0-auc:0.80506\n",
      "[44]\tvalidation_0-auc:0.80729\n",
      "[45]\tvalidation_0-auc:0.80825\n",
      "[46]\tvalidation_0-auc:0.81036\n",
      "[47]\tvalidation_0-auc:0.81159\n",
      "[48]\tvalidation_0-auc:0.81301\n",
      "[49]\tvalidation_0-auc:0.81328\n",
      "[50]\tvalidation_0-auc:0.81460\n",
      "[51]\tvalidation_0-auc:0.81564\n",
      "[52]\tvalidation_0-auc:0.81712\n",
      "[53]\tvalidation_0-auc:0.81810\n",
      "[54]\tvalidation_0-auc:0.81886\n",
      "[55]\tvalidation_0-auc:0.81934\n",
      "[56]\tvalidation_0-auc:0.82099\n",
      "[57]\tvalidation_0-auc:0.82219\n",
      "[58]\tvalidation_0-auc:0.82296\n",
      "[59]\tvalidation_0-auc:0.82386\n",
      "[60]\tvalidation_0-auc:0.82416\n",
      "[61]\tvalidation_0-auc:0.82501\n",
      "[62]\tvalidation_0-auc:0.82596\n",
      "[63]\tvalidation_0-auc:0.82670\n",
      "[64]\tvalidation_0-auc:0.82740\n",
      "[65]\tvalidation_0-auc:0.82776\n",
      "[66]\tvalidation_0-auc:0.82899\n",
      "[67]\tvalidation_0-auc:0.83026\n",
      "[68]\tvalidation_0-auc:0.83109\n",
      "[69]\tvalidation_0-auc:0.83233\n",
      "[70]\tvalidation_0-auc:0.83327\n",
      "[71]\tvalidation_0-auc:0.83395\n",
      "[72]\tvalidation_0-auc:0.83425\n",
      "[73]\tvalidation_0-auc:0.83501\n",
      "[74]\tvalidation_0-auc:0.83578\n",
      "[75]\tvalidation_0-auc:0.83648\n",
      "[76]\tvalidation_0-auc:0.83733\n",
      "[77]\tvalidation_0-auc:0.83770\n",
      "[78]\tvalidation_0-auc:0.83838\n",
      "[79]\tvalidation_0-auc:0.83918\n",
      "[80]\tvalidation_0-auc:0.83968\n",
      "[81]\tvalidation_0-auc:0.83996\n",
      "[82]\tvalidation_0-auc:0.84024\n",
      "[83]\tvalidation_0-auc:0.84063\n",
      "[84]\tvalidation_0-auc:0.84122\n",
      "[85]\tvalidation_0-auc:0.84179\n",
      "[86]\tvalidation_0-auc:0.84236\n",
      "[87]\tvalidation_0-auc:0.84315\n",
      "[88]\tvalidation_0-auc:0.84349\n",
      "[89]\tvalidation_0-auc:0.84401\n",
      "[90]\tvalidation_0-auc:0.84417\n",
      "[91]\tvalidation_0-auc:0.84505\n",
      "[92]\tvalidation_0-auc:0.84548\n",
      "[93]\tvalidation_0-auc:0.84581\n",
      "[94]\tvalidation_0-auc:0.84623\n",
      "[95]\tvalidation_0-auc:0.84662\n",
      "[96]\tvalidation_0-auc:0.84705\n",
      "[97]\tvalidation_0-auc:0.84740\n",
      "[98]\tvalidation_0-auc:0.84775\n",
      "[99]\tvalidation_0-auc:0.84808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=5, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
       "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "              use_label_encoder=False, validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train, y_train,\n",
    "           early_stopping_rounds=10,                # stop if model performance doesn't change for 10 iterations\n",
    "           eval_set = test_set,\n",
    "           verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditg\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of Train: 0.9144 and Test: 0.8481\n",
      "F1 score of Train: 0.2295 and Test: 0.1106\n",
      "Train confusion matrix \n",
      " [[143970     29]\n",
      " [ 13923   2078]] \n",
      "\n",
      "Test confusion matrix \n",
      " [[35865    38]\n",
      " [ 3855   242]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model3.predict(X_train)\n",
    "y_test_pred  = model3.predict(X_test)\n",
    "\n",
    "y_train_prob = model3.predict_proba(X_train)[:,1]\n",
    "y_test_prob  = model3.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('AUC score of Train: {:.4f} and Test: {:.4f}'.format(roc_auc_score(y_train, y_train_prob),\n",
    "                                                           roc_auc_score(y_test, y_test_prob)))\n",
    "\n",
    "print('F1 score of Train: {:.4f} and Test: {:.4f}'.format(f1_score(y_train, y_train_pred),\n",
    "                                                          f1_score(y_test, y_test_pred)))\n",
    "\n",
    "print('Train confusion matrix \\n', confusion_matrix(y_train, y_train_pred), '\\n')\n",
    "print('Test confusion matrix \\n', confusion_matrix(y_test, y_test_pred), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 500 runs\n",
    "# precision = 8524/(8524+506)\n",
    "# recall = 8524/(8524+7477)\n",
    "# 2*precision*recall/(precision + recall) # 0.6810\n",
    "\n",
    "\n",
    "# precision = 1286/(1286+381)\n",
    "# recall = 1286/(1286+2811  )\n",
    "# 2*precision*arecall/(precision + recall) # 0.4462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
